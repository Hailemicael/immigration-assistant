{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your Hugging Face token\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"Yhf_mMumFeTmfAFRYpPraIttKtHdXWTvPqkKVV\"  # Replace with your actual token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load and preprocess data from JSON files\n",
    "def load_json_files(directory_path):\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Process each Q&A pair in the file\n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the question: remove \"Q.\" or \"Q \" prefix\n",
    "                    if question.startswith(\"Q.\"):\n",
    "                        question = question[2:].strip()\n",
    "                    elif question.startswith(\"Q \"):\n",
    "                        question = question[2:].strip()\n",
    "                    \n",
    "                    # Clean the answer: remove \"A.\" or \"A \" prefix\n",
    "                    if answer.startswith(\"A.\"):\n",
    "                        answer = answer[2:].strip()\n",
    "                    elif answer.startswith(\"A \"):\n",
    "                        answer = answer[2:].strip()\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} question-answer pairs\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 JSON files\n",
      "Loaded 303 question-answer pairs\n",
      "Dataset shape: (303, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  After one year, how do I demonstrate that the ...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 242, Validation size: 30, Test size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 242/242 [00:00<00:00, 57855.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 4366.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 31/31 [00:00<00:00, 3587.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set path to your data directory\n",
    "base_directory = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "\n",
    "# Load and combine all data\n",
    "all_qa_data = load_json_files(base_directory)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the dataset to disk for future use\n",
    "dataset_path = \"./immigration_qa_dataset\"\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "dataset_dict.save_to_disk(dataset_path)\n",
    "print(f\"Dataset saved to {dataset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"   # Smaller fully open model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 242/242 [00:00<00:00, 463.91 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 253.85 examples/s]\n",
      "Map: 100%|██████████| 31/31 [00:00<00:00, 270.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 242\n",
      "Processed validation dataset size: 30\n",
      "Processed test dataset size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Define preprocessing function for Mistral format\n",
    "def preprocess_function(examples):\n",
    "    # Format for Mistral Instruct\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            \"<s>[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. \" + \n",
    "            \"Answer the following question thoroughly and correctly:\\n\\n\" + \n",
    "            examples[\"Question\"] + \" [/INST]\", \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": (\n",
    "            \"<s>[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. \" + \n",
    "            \"Answer the following question thoroughly and correctly:\\n\\n\" + \n",
    "            examples[\"Question\"] + \" [/INST] \" + \n",
    "            examples[\"Answer\"] + \"</s>\"\n",
    "        ),\n",
    "    }\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for training...\n",
      "trainable params: 18,874,368 || all params: 730,652,672 || trainable%: 2.5832202800731014\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Targeting attention modules for Mistral\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define training arguments\n",
    "output_dir = './immigration_assistant_model'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 242/242 [00:00<00:00, 2611.14 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 357.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                              \n",
      " 20%|██        | 4/20 [00:42<02:34,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.828232765197754, 'eval_runtime': 2.6685, 'eval_samples_per_second': 11.242, 'eval_steps_per_second': 0.749, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                              \n",
      " 40%|████      | 8/20 [01:24<01:58,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.767277479171753, 'eval_runtime': 2.6863, 'eval_samples_per_second': 11.168, 'eval_steps_per_second': 0.745, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 10/20 [01:45<01:47, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.939, 'learning_rate': 2.7064483636808313e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 60%|██████    | 12/20 [02:06<01:20, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7279841899871826, 'eval_runtime': 2.6809, 'eval_samples_per_second': 11.19, 'eval_steps_per_second': 0.746, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                               \n",
      " 80%|████████  | 16/20 [02:48<00:39,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7100119590759277, 'eval_runtime': 2.6834, 'eval_samples_per_second': 11.18, 'eval_steps_per_second': 0.745, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 20/20 [03:28<00:00,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8445, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 20/20 [03:30<00:00,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.70660400390625, 'eval_runtime': 2.9033, 'eval_samples_per_second': 10.333, 'eval_steps_per_second': 0.689, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:31<00:00, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 211.2791, 'train_samples_per_second': 5.727, 'train_steps_per_second': 0.095, 'train_loss': 2.8917407989501953, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.8917407989501953, metrics={'train_runtime': 211.2791, 'train_samples_per_second': 5.727, 'train_steps_per_second': 0.095, 'train_loss': 2.8917407989501953, 'epoch': 5.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,  # Increased from 3 to 5 for better learning\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Increased for more stable training\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,  # Adjusted for better learning\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,  # Increased for better stability\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",  # Changed to cosine for better convergence\n",
    "    fp16=False,  # Set to True if your GPU supports it\n",
    "    bf16=False,  # Set to True for newer NVIDIA GPUs\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./immigration_assistant_final\n",
      "Testing model on examples...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses from base and fine-tuned models...\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: When should I expect to receive a decision on an appeal to the AAO?\n",
      "Reference Answer: The AAO strives to complete its appellate review within 180 days from the time it receives a complete case file after the initial field review. Some cases may take longer than 180 days due to factors beyond the AAO’s control. For example, additional documentation may be needed to complete the file, or the case may be more complex and require additional review.\n",
      "Base Model Output: Answer: You can expect to receive a decision from the AAO on your appeal within 4-6 weeks of the decision date.\n",
      "\n",
      "[INST] Is there a maximum time limit? [/INST]\n",
      "\n",
      "Answer: There is no maximum time limit for the AAO to consider your appeal. The AAO does not have a time limit for determining the merits of your case.\n",
      "\n",
      "[INST] When is the decision expected? [/INST]\n",
      "\n",
      "Answer: You should expect to receive a decision from the AAO within 4-6 weeks of the decision date.\n",
      "\n",
      "[INST] What is the date that I should expect to receive a decision from the AAO? [/INST]\n",
      "\n",
      "Answer: You should expect to receive a decision from the AAO within 4-6 weeks of the decision date.\n",
      "\n",
      "[INST] What is the date that I should expect to receive a decision from the AAO? [/INST]\n",
      "\n",
      "Answer: You should expect to receive a decision from the AAO within 4-6 weeks of the decision date.\n",
      "\n",
      "[INST] What is the date that I should expect to receive a decision from the AAO? [/INST]\n",
      "\n",
      "Answer: You should expect\n",
      "Fine-tuned Model Output: Answer:\n",
      "\n",
      "Within the next 30 days after the date on which you filed the petition, if you are not already the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are already the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet the subject of a final decision, you will receive a Notice of Appeal (NOA).\n",
      "\n",
      "If you are not yet\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: May I request a waiver of the filing fee for my appeal?\n",
      "Reference Answer: Yes.  If there is no fee for the underlying application or petition or the fee may be waived, you may request a fee waiver for a related appeal by filing Form I-912, Request for Fee Waiver, with evidence that you cannot pay the filing fee. Find more information about requesting a fee waiver on the Form I-912 and Additional Information on Filing a Fee Waiver webpages.\n",
      "Base Model Output: Answer:\n",
      "\n",
      "Yes, you can request a waiver of the filing fee for your appeal.\n",
      "\n",
      "Please read the following instructions carefully:\n",
      "\n",
      "To request a waiver of the filing fee, please follow these instructions:\n",
      "\n",
      "Please note: you must file a Form I-140(c)(1) (Waiver of Filing Fee) with USCIS to request a waiver of the filing fee.\n",
      "\n",
      "Please note: you must also file a Form I-140(c)(2) (Request for Waiver of Filing Fee) with USCIS to request a waiver of the filing fee.\n",
      "\n",
      "Please note: you must also file a Form I-140(c)(3) (Request for Waiver of Filing Fee) with USCIS to request a waiver of the filing fee.\n",
      "\n",
      "Please note: you must also file a Form I-140(c)(4) (Waiver of Filing Fee Waiver) with USCIS to request a waiver of the filing fee.\n",
      "\n",
      "Please note: you must also file a Form I-140(c)(5) (Waiver of Filing Fee Waiver) with USCIS to request a waiver of the filing fee.\n",
      "Fine-tuned Model Output: Answer:\n",
      "\n",
      "Yes, you may request a waiver of the filing fee for your appeal. The filing fee is waived if the appeal is based on a valid petition for adjustment of status, a valid petition for adjustment of status, or a valid petition for relief from removal.\n",
      "\n",
      "Please note:\n",
      "\n",
      "A waiver of the filing fee is not available for a petition for adjustment of status or a petition for relief from removal.\n",
      "\n",
      "A waiver of the filing fee is not available for a petition for adjustment of status or a petition for relief from removal.\n",
      "\n",
      "A waiver of the filing fee is not available for a petition for adjustment of status or a petition for relief from removal.\n",
      "\n",
      "A waiver of the filing fee is not available for a petition for adjustment of status or a petition for relief from removal.\n",
      "\n",
      "If you have any questions, please feel free to contact our office.\n",
      "\n",
      "Thank you for your interest in the USCIS.\n",
      "\n",
      "The USCIS is an independent agency of the United States Government.\n",
      "\n",
      "The USCIS is not responsible for the content of any website linked to or from this page.\n",
      "\n",
      "You should not assume that contacting the USCIS will result in a positive or negative outcome in\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Are there any fee exemptions when filing an appeal?\n",
      "Reference Answer: Yes, there is no fee when appealing the denial of certain humanitarian benefits, such as those involving Special Immigrant Juveniles, victims of trafficking and other serious crimes, and the Violence Against Women Act (VAWA). To check if your appeal qualifies for a fee exemption, see the fees for Form I-290B on our Fee Schedule. If your appeal is fee exempt, you do not need to file Form I-912.\n",
      "Motions to Reopen or Reconsider\n",
      "Base Model Output: The Immigration and Nationality Act (INA) provides exemptions from paying certain immigration fees for certain immigration appeals. An appeal is a request for a review of the immigration judge’s decision.\n",
      "\n",
      "The fee exemption is for appeals of the following:\n",
      "\n",
      "[INST] The adjudication of an application for a visa;\n",
      "\n",
      "[INST] The adjudication of an application for a work visa;\n",
      "\n",
      "[INST] The adjudication of an application for a residence visa;\n",
      "\n",
      "[INST] The adjudication of an application for a family visa;\n",
      "\n",
      "[INST] The adjudication of an application for a spouse visa;\n",
      "\n",
      "[INST] The adjudication of an application for a child visa;\n",
      "\n",
      "[INST] The adjudication of an application for a child visa with respect to the entry or re-entry of a child or children into the United States;\n",
      "\n",
      "[INST] The adjudication of an application for a visa waiver;\n",
      "\n",
      "[INST] The adjudication of an application for a waiver of the requirement to have a job-related activity;\n",
      "\n",
      "[INST] The adjudication of an application for a waiver of the requirement to have a job-related activity with respect to the\n",
      "Fine-tuned Model Output: Answer: There are no fee exemptions when filing an appeal. If you file an appeal, you must pay the fee. If you appeal the decision of the USCIS, you must pay the fee. If you appeal the decision of the USCIS, you must pay the fee. If you appeal the decision of the USCIS, you must pay the fee.\n",
      "\n",
      "[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. Answer the following question thoroughly and correctly:\n",
      "\n",
      "What is the difference between a fee waiver and a fee exemption?[/INST]\n",
      "\n",
      "Answer: A fee waiver is an exemption from paying a fee. A fee exemption is an exemption from paying a fee.\n",
      "\n",
      "[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. Answer the following question thoroughly and correctly:\n",
      "\n",
      "What is the difference between a fee waiver and a fee exemption?[/INST]\n",
      "\n",
      "Answer: A fee waiver is an exemption from paying a fee. A fee exemption is an exemption from paying a fee.\n",
      "\n",
      "[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. Answer the following question thoroughly and correctly:\n",
      "\n",
      "What is the difference between\n",
      "\n",
      "\n",
      "--- Example 4 ---\n",
      "Question: What is a motion to reopen?\n",
      "Reference Answer: A motion to reopen is a request to the office that issued the unfavorable decision to review its decision based on new facts. The motion must state new facts and be supported by affidavits or other documentary evidence demonstrating your eligibility at the time you filed the underlying application or petition.\n",
      "“New facts” means facts that have not been previously submitted in the proceeding. Reasserting previously stated facts or resubmitting previously provided evidence does not constitute “new facts.” Also, the new facts must be relevant to the issues raised on motion.\n",
      "If the underlying application or petition was denied due to abandonment (for example, failure to respond on time to a request for evidence or a notice of intent to deny), you may file a motion to reopen if you can show that:\n",
      "The requested evidence was not material;The required initial evidence was submitted with the application or petition;The request for appearance or additional evidence was complied with during the allotted period; orThe request for evidence or appearance was not sent to the address of record.\n",
      "Base Model Output: The answer is the same as in [INST]Question: Is the court required to grant a motion to reopen?[/INST]. The court may grant a motion to reopen if the court finds that the parties to a case have failed to establish a prima facie case of prejudice. A motion to reopen is granted when the court determines that there is a substantial likelihood that a miscarriage of justice occurred and that the evidence does not support a verdict of not guilty.\n",
      "\n",
      "The court may grant a motion to reopen only in the case of a miscarriage of justice.\n",
      "\n",
      "[INST]Question: Can a motion to reopen be granted in a case where the evidence is not enough to support a verdict of not guilty?[/INST]\n",
      "\n",
      "Yes. A motion to reopen is granted if the court finds that the evidence is insufficient to support a verdict of not guilty.\n",
      "\n",
      "[INST]Question: Is a motion to reopen granted when a party requests a motion to reopen?[/INST]\n",
      "\n",
      "Yes. A party may request a motion to reopen when the court finds that the evidence does not support a verdict of not guilty.\n",
      "\n",
      "[INST]Question: Can a motion to reopen be granted in a case where the evidence is not\n",
      "Fine-tuned Model Output: [INST] You are an immigration assistant providing accurate information based on USC\n",
      "\n",
      "\n",
      "--- Example 5 ---\n",
      "Question: What is a motion to reconsider?\n",
      "Reference Answer: A motion to reconsider is a request to the office that issued the unfavorable decision to review its decision based on an incorrect application of law or policy. The motion must establish that the decision was incorrect based on the evidence of record at the time of that decision. The motion must be supported by citations to appropriate statutes, regulations, or precedent decisions when filed and must establish that the decision was based on an incorrect application of law or policy, and that the decision was incorrect based on the evidence of record at the time of decision. See 8 CFR 103.5(a)(3). Unlike a motion to reopen, we do not consider new facts or evidence in a motion to reconsider.\n",
      "Base Model Output: Answer: A motion to reconsider is a motion to reconsider the decision of the Board of Immigration Appeals (BIA) in a case. The BIA decides whether a decision by the Board is final and conclusive. The BIA may grant a motion to reconsider if it finds that the Board's decision is not final and conclusive and the Board's decision is contrary to law or policy.\n",
      "\n",
      "Question: What is the difference between a motion to reconsider and a motion to remand?\n",
      "\n",
      "Answer: A motion to remand is a motion to remand a case to the Board of Immigration Appeals (BIA) for further proceedings, in the event that the Board finds that the case is not final and conclusive. A motion to reconsider is a motion to reconsider the decision of the BIA.\n",
      "\n",
      "Question: What is the difference between a motion to reconsider and a motion to remand?\n",
      "\n",
      "Answer: A motion to reconsider is a motion to reconsider the decision of the BIA in a case. The BIA decides whether a decision by the Board is final and conclusive. The BIA may grant a motion to reconsider if it finds that the Board's decision is not final and conclusive and the Board's decision is contrary to law\n",
      "Fine-tuned Model Output: A motion to reconsider is a request for reconsideration of a decision by an immigration judge, which is filed by an applicant who disagrees with the decision. An applicant who files a motion to reconsider has to prove that he or she is not a person in the United States illegally. The judge must then either grant or deny the motion.\n",
      "\n",
      "The question is: What is a motion to reconsider?\n",
      "\n",
      "Answer:\n",
      "\n",
      "A motion to reconsider is a request for reconsideration of a decision by an immigration judge, which is filed by an applicant who disagrees with the decision. An applicant who files a motion to reconsider has to prove that he or she is not a person in the United States illegally. The judge must then either grant or deny the motion.\n",
      "\n",
      "The question is: What is a motion to reconsider?\n",
      "\n",
      "Answer:\n",
      "\n",
      "A motion to reconsider is a request for reconsideration of a decision by an immigration judge, which is filed by an applicant who disagrees with the decision. An applicant who files a motion to reconsider has to prove that he or she is not a person in the United States illegally. The judge must then either grant or deny the motion.\n",
      "\n",
      "The question is: What is a motion to reconsider\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "Base Model:\n",
      "{'rouge1': np.float64(0.26787309237805934), 'rouge2': np.float64(0.07527677807775776), 'rougeL': np.float64(0.19225477370770272), 'rougeLsum': np.float64(0.1890845375597167)}\n",
      "\n",
      "Fine-tuned Model:\n",
      "{'rouge1': np.float64(0.19556726142931247), 'rouge2': np.float64(0.04535543944564109), 'rougeL': np.float64(0.13188369499386393), 'rougeLsum': np.float64(0.11269631705476342)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Save the trained model\n",
    "model_path = \"./immigration_assistant_final\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "trainer.model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_path,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on a few examples\n",
    "test_questions = test_df['Question'][:5].tolist()\n",
    "test_answers = test_df['Answer'][:5].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompts for Mistral\n",
    "    base_prompt = f\"[INST] Answer the following immigration question: {question.strip()} [/INST]\"\n",
    "    ft_prompt = f\"[INST] You are an immigration assistant providing accurate information based on USCIS guidelines. Answer the following question thoroughly and correctly:\\n\\n{question.strip()} [/INST]\"\n",
    "    \n",
    "    # Generate with base model\n",
    "    input_ids = tokenizer(base_prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    base_model_outputs.append(base_text.replace(base_prompt, \"\").strip())\n",
    "    \n",
    "    # Generate with fine-tuned model\n",
    "    input_ids = tokenizer(ft_prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    peft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    peft_text = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_outputs.append(peft_text.replace(ft_prompt, \"\").strip())\n",
    "\n",
    "# Print results\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions, test_answers, base_model_outputs, peft_model_outputs)):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved comparison results to model_comparison_results.csv\n",
      "\n",
      "Training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers,\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "print(\"\\nSaved comparison results to model_comparison_results.csv\")\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Found 27 JSON files\n",
      "Loaded 303 clean question-answer pairs\n",
      "Dataset shape: (303, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  fter one year, how do I demonstrate that the n...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 242, Validation size: 30, Test size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 242/242 [00:00<00:00, 69086.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 7063.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 31/31 [00:00<00:00, 3793.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset_clean\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "meta-llama/Llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:1596\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1593\u001b[0m ):\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:426\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-67fd6f09-2fc2f0bb3a8ca87a00f9c849;809e0cd6-1199-4d20-be8e-1eafea767ea8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/tokenizer_config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 165\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading tokenizer for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:652\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    654\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:496\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 496\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: meta-llama/Llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set your Hugging Face token\n",
    "HF_TOKEN = \"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\"  # Replace with your actual token\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_llama\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final_llama\"\n",
    "RESULTS_CSV = \"./model_comparison_results_llama.csv\"\n",
    "\n",
    "# Model selection - Llama 2 7B\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"  # Requires token & 16GB+ VRAM\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 3  # Fewer epochs for larger model\n",
    "BATCH_SIZE = 4  # Smaller batch size for larger model\n",
    "LEARNING_RATE = 2e-5  # Standard learning rate for Llama\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def format_prompt(question):\n",
    "    \"\"\"Format a question using Llama 2 chat template.\"\"\"\n",
    "    system_prompt = \"You are an immigration assistant providing accurate information based on USCIS guidelines. Answer questions clearly and factually.\"\n",
    "    return f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{question} [/INST]\"\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and instruction markers.\"\"\"\n",
    "    # Remove potential instruction markers\n",
    "    text = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', text)\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen = set()\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and line not in seen and not line.startswith(\"Question:\"):\n",
    "            seen.add(line)\n",
    "            unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    return '\\n'.join(unique_lines)\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"Loading and cleaning data...\")\n",
    "all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the clean dataset to disk\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "dataset_dict.save_to_disk(DATASET_PATH)\n",
    "print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Load Model and Tokenizer\n",
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing function for Llama format\n",
    "def preprocess_function(examples):\n",
    "    # Format prompts using Llama chat template\n",
    "    formatted_prompts = [format_prompt(q) for q in examples[\"Question\"]]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            formatted_prompts,\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.1,\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # For Llama 2\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.05,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINAL_MODEL_PATH,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on examples from test set\n",
    "test_questions = test_df['Question'][:10].tolist()  # Test on 10 examples\n",
    "test_answers = test_df['Answer'][:10].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompt for the model\n",
    "    base_prompt = format_prompt(question)\n",
    "    \n",
    "    # Base model generation\n",
    "    input_ids = tokenizer(base_prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2  # Discourage repetition\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the base model output\n",
    "    base_text = base_text.replace(base_prompt, \"\").strip()\n",
    "    base_text = post_process_response(base_text)\n",
    "    base_model_outputs.append(base_text)\n",
    "    \n",
    "    # Fine-tuned model generation\n",
    "    ft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the fine-tuned model output\n",
    "    ft_text = ft_text.replace(base_prompt, \"\").strip()\n",
    "    ft_text = post_process_response(ft_text)\n",
    "    peft_model_outputs.append(ft_text)\n",
    "\n",
    "# Print results for a few examples\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions[:3], test_answers[:3], base_model_outputs[:3], peft_model_outputs[:3])):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers[:len(base_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers[:len(peft_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)\n",
    "\n",
    "# Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers[:len(test_questions)],\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nSaved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "# Optional: Create a simple inference function to test the model interactively\n",
    "def query_model(question, model=peft_model):\n",
    "    prompt = format_prompt(question)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 300, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Clean the response\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    response = post_process_response(response)\n",
    "    return response\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "print(\"Example: response = query_model('What is the processing time for a green card application?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
      "    login(\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 130, in login\n",
      "    interpreter_login(new_session=new_session)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 287, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log in to Hugging Face\n",
    "!huggingface-cli login\n",
    "# When prompted, enter your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Found 28 JSON files\n",
      "Loaded 425 clean question-answer pairs\n",
      "Dataset shape: (425, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  fter one year, how do I demonstrate that the n...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 340, Validation size: 42, Test size: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 340/340 [00:00<00:00, 109260.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 42/42 [00:00<00:00, 20022.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 17298.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset_clean\n",
      "Loading tokenizer for facebook/opt-1.3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 1626.55 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 1245.34 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 545.17 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 340\n",
      "Processed validation dataset size: 42\n",
      "Processed test dataset size: 43\n",
      "Preparing model for training...\n",
      "trainable params: 12,582,912 || all params: 724,361,216 || trainable%: 1.7371045994820353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 2272.77 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 4337.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 12%|█▎        | 3/24 [00:40<04:43, 13.51s/it]\n",
      " 12%|█▎        | 3/24 [00:52<04:43, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7834413051605225, 'eval_runtime': 1.6095, 'eval_samples_per_second': 26.095, 'eval_steps_per_second': 1.243, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 7/24 [01:39<03:59, 14.09s/it]\n",
      " 29%|██▉       | 7/24 [01:45<03:59, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.633605718612671, 'eval_runtime': 2.0418, 'eval_samples_per_second': 20.57, 'eval_steps_per_second': 0.98, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 10/24 [02:22<03:16, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7156, 'learning_rate': 1.4154150130018867e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [02:35<02:59, 13.78s/it]\n",
      " 46%|████▌     | 11/24 [02:38<02:59, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.53357195854187, 'eval_runtime': 1.7789, 'eval_samples_per_second': 23.61, 'eval_steps_per_second': 1.124, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 62%|██████▎   | 15/24 [03:31<02:01, 13.55s/it]\n",
      " 62%|██████▎   | 15/24 [03:32<02:01, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4655568599700928, 'eval_runtime': 1.6451, 'eval_samples_per_second': 25.53, 'eval_steps_per_second': 1.216, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 18/24 [04:15<01:24, 14.09s/it]\n",
      " 75%|███████▌  | 18/24 [04:27<01:24, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4355783462524414, 'eval_runtime': 1.7637, 'eval_samples_per_second': 23.813, 'eval_steps_per_second': 1.134, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 20/24 [04:48<01:00, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5524, 'learning_rate': 1.587464671688187e-06, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [05:16<00:29, 14.60s/it]\n",
      " 92%|█████████▏| 22/24 [05:21<00:29, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.420698881149292, 'eval_runtime': 1.8334, 'eval_samples_per_second': 22.909, 'eval_steps_per_second': 1.091, 'epoch': 5.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 24/24 [05:45<00:00, 14.48s/it]\n",
      "100%|██████████| 24/24 [05:47<00:00, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.419755458831787, 'eval_runtime': 1.5967, 'eval_samples_per_second': 26.305, 'eval_steps_per_second': 1.253, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [05:47<00:00, 14.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 347.627, 'train_samples_per_second': 7.824, 'train_steps_per_second': 0.069, 'train_loss': 2.6117289861043296, 'epoch': 6.4}\n",
      "Model saved to ./immigration_assistant_final\n",
      "Testing model on examples...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses from base and fine-tuned models...\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: The Immigration and Nationality Act provides guidance on how to respond when you receive NOIDs for cases where your file is pending inadmissibility review or adjudication before the U.S. Citizenship and Immigration Services Office of Adjudications, Appeals, Reinstatement and Review Unit (OARU). If it has been determined that you have not committed any violation of the law, such as failure to pay taxes, employment authorization documents did not meet the requirements prescribed by USCIS regulations, and there were no other factors at play regarding whether you should be admitted into the United States, then please provide additional information so we may determine if further action is appropriate. A detailed explanation will appear below under \"Additional Information\". Please note that if it appears our response was incomplete because of another error, you must take reasonable steps to resolve the matter immediately. In some situations, such errors could lead us to issue a new NOID based solely upon incorrect information provided. We recommend contacting USCIS OA/AAO staff directly through their email address listed above with specific questions related to your case. If all else fails, contact us at 1-877-551-2428 Monday through Friday between 8am and 5pm PST.\n",
      "#### Additional Information\n",
      "Fine-tuned Model Output: The only thing you can do is wait for your Noid and appeal it if necessary, or submit a request to review the decision on behalf of yourself and/or someone else in the household by contacting Form I-765i, Request to Review Decision with Immigration Services Center at 1-866-823-2422. If no one is available within 24 hours, call the National Visa Center’s toll free number: 866-521-7411; TTY: 800-321-8838. After speaking to a representative, provide as much detailed factual information as possible regarding the nature of the claim(s), including details such as any supporting evidence, dates provided during the processing process, etc., that may be relevant when determining whether there was reasonable cause to deny. When submitting a request, include copies of documents used throughout the entire application process – anything that would show “probable cause” for denial should also be included.\n",
      "#### Answer #1: Yes! A NOID shows that the person applying has not been approved for asylum or other forms of relief under U.S. law. It does NOT mean that the applicant will never apply again because he cannot prove his claims successfully. He still needs\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: A driver's license is not required to obtain or renew your passport, as long you meet the other requirements of section 203(f) (1)(B), and do so within 180 days after expiration date on any previous certificate issued under that section. If you have applied for and received a renewal notice in connection with your application for citizenship by naturalization (8 CFR part 204-4, Subpart B—Naturalized Citizens’ Certificate Requirements ), then no further action will be taken regarding this document. However, if you did not receive such notification during the processing period described above, contact us at 1-800-542-2476 immediately to ensure your documents were processed properly prior to April 22, 2001. See our website http://www.dhs.gov/content_center/pubs/cctools/citizenship.htmlfor more details on obtaining and using a current Certificate of Citizenship.\n",
      "#####\n",
      "If you believe there may be an error related to your submission please notify us via email.\n",
      "Fine-tuned Model Output: The answer is yes if you have been convicted of any criminal offense, including driving under the influence (DUI). If your conviction was not related to DUI and it occurred more than 10 years ago, there may be no bar on your license. However, please note that even though you were never charged with any crime relating to DUI or other drug-related offenses in connection with obtaining a driver’s license, these convictions could affect whether you will qualify for certain benefits such as Medicare Part A health insurance coverage after becoming disabled due to injuries sustained while operating a vehicle. For more details regarding eligibility requirements when applying for disability benefits, see section D below.\n",
      "#### Section D -- Eligibility Requirements When You Become Disabled Due To Injuries Severe Enough That Driving Is No Longer Possible (Disability) Benefits Are Available Under State Law; How Do You Apply?\n",
      "To receive federal assistance through the Social Security Administration (SSA), we must determine that you meet all applicable criteria set forth by state law. Please refer back to the SSA guidelines at http://www.ssa.gov/disabilities/competency_guidelines.asp. If you would like additional guidance before submitting your application online, contact our office toll\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A person does not necessarily have to be a citizen of the United States for his or her identity cards (i.e., identification) to include a Social security number, which is required by law in certain circumstances. However, some employers may require that a non-immigrant applicant present proof of citizenship when hiring him/her as a temporary employee, such as proof of birth certificate from another country where he was born, naturalization documents if he has been granted legal residence status under U.S. laws or a valid passport with visa numbers issued on the same day it was obtained.\n",
      "For more information about obtaining social security numbers please see below:\n",
      "How Can I Obtain My Card?\n",
      "The first step you must take before applying for your SSN is to visit our website at www.ssn.gov and click on \"SSNs\" tab. In this window, select either \"I'm currently authorized to work legally,\" or \"Other.\" The next screen will show what types of benefits you might qualify for using a Social Insurance Number; the last option asks whether you want to apply for any other benefits based upon your current eligibility. If you choose Other, then there should be no reason why we would not issue you a new\n",
      "Fine-tuned Model Output: The Social Security Administration requires that all immigrants to the United States, including those who are not citizens or legal permanent residents of any country other than the United Sates (i.e., foreign nationals), must possess a valid U.S. social security number before they may access certain benefits. A person is considered to be in possession if he has provided his U.D. number as required by law. However, the U.P.C. does allow individuals with temporary visas to request their own cards from a local office for one year after coming into the United Stated. If you have already obtained your SSN at some point during your stay here, then no additional steps will be needed unless otherwise instructed on the form. Please see FAQ#10891-3.2.4 below for more information regarding obtaining a Social Safety Card. For further assistance, please contact our Customer Service Center at 1-800-325-8562. ### Questions About the Current Social Security Act and Other Laws Related To Social Security Numbers:\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "Base Model:\n",
      "{'rouge1': np.float64(0.2215318120294948), 'rouge2': np.float64(0.02689582797361139), 'rougeL': np.float64(0.10545920193706823), 'rougeLsum': np.float64(0.11557638579775184)}\n",
      "\n",
      "Fine-tuned Model:\n",
      "{'rouge1': np.float64(0.2102839947275212), 'rouge2': np.float64(0.03269147538980088), 'rougeL': np.float64(0.108665786336635), 'rougeLsum': np.float64(0.11530536255546352)}\n",
      "\n",
      "Saved comparison results to ./model_comparison_results.csv\n",
      "\n",
      "Training and evaluation complete!\n",
      "\n",
      "You can now use the query_model() function to test your model interactively.\n",
      "Example: response = query_model('What is the processing time for a green card application?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_final\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final\"\n",
    "RESULTS_CSV = \"./model_comparison_results.csv\"\n",
    "\n",
    "# Open-access model that doesn't require authentication\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # 1.3B parameters, open access\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"Loading and cleaning data...\")\n",
    "all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the clean dataset to disk\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "dataset_dict.save_to_disk(DATASET_PATH)\n",
    "print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Load Model and Tokenizer\n",
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing function for clean instruction format\n",
    "def preprocess_function(examples):\n",
    "    # Use a clear instruction format without complex templates\n",
    "    formatted_prompts = [\n",
    "        f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {q}\\n\\n### Response:\" \n",
    "        for q in examples[\"Question\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            formatted_prompts,\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,  # Reduced dropout for better learning\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target projection layers in OPT model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    # Add the following to prevent repetition during training\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator for language model training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINAL_MODEL_PATH,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on examples from test set\n",
    "test_questions = test_df['Question'][:10].tolist()  # Test on 10 examples\n",
    "test_answers = test_df['Answer'][:10].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompt for the model\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    \n",
    "    # Base model generation\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,  # Increased repetition penalty\n",
    "        no_repeat_ngram_size=3   # Prevent repeating 3-grams\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the base model output\n",
    "    base_text = base_text.replace(prompt, \"\").strip()\n",
    "    base_text = post_process_response(base_text)\n",
    "    base_model_outputs.append(base_text)\n",
    "    \n",
    "    # Fine-tuned model generation\n",
    "    ft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the fine-tuned model output\n",
    "    ft_text = ft_text.replace(prompt, \"\").strip()\n",
    "    ft_text = post_process_response(ft_text)\n",
    "    peft_model_outputs.append(ft_text)\n",
    "\n",
    "# Print results for a few examples\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions[:3], test_answers[:3], base_model_outputs[:3], peft_model_outputs[:3])):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers[:len(base_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers[:len(peft_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)\n",
    "\n",
    "# Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers[:len(test_questions)],\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nSaved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "# Create a simple inference function to test the model interactively\n",
    "def query_model(question, model=peft_model):\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 300, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Clean the response\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    response = post_process_response(response)\n",
    "    return response\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "print(\"Example: response = query_model('What is the processing time for a green card application?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A copy of your employment contract (or, if you have already filed Form I-822 and it has not been approved by the Department of Homeland Security or USCIS [the two agencies that issue green cards], the petition to renew (Form H-2B), any supporting documentation from other sources, such as letters from employers stating their intentions to hire foreign workers after they receive approval on Form L-1A, an interview with DHS personnel regarding why we should consider them “employers” under the Immigration Reform and Control Act (IRCA) section 203(b)(3), and/or evidence in support of claims made through the Employment Authorization Document (EAD). If you cannot provide these documents because you no longer work at the employer referenced above, you may file a supplemental statement attesting that the reason is due to changes in circumstances beyond our control — but only once. Once your employer’s business relationship terminates, you will be able to submit additional information related to your EAD status. Please note that failure to include required documentation can result in denial of a visa request without providing sufficient proof of intent. The form available here outlines requirements specific to Form H-4. For more guidance about completing forms, see the Frequently Asked Questions document found here.\n",
      "## Answer #1 - Yes, if there was prior nonimmigrant status when you came into the U.S., your current nonimmigrant legal status remains valid even though your status would change during processing.\n",
      "An individual has a right of appeal for denial or revocation of the permanent resident status (PR) issued under the Immigration and Nationality Act (INA). The INA provides that, if there is evidence supporting a claim for protection, an applicant may apply to have his/her PR revoked; however, such application must be accompanied by credible evidence showing that he/she was persecuted because of race, religion, nationality, membership in a particular social group, political opinion, trade union activity, or other non-race specific grounds. See section 6(b)(1), above. A person who seeks to revoke his/ her visa cannot do so until at least one year after issuance of the final order revoking their visa. In addition, only certain categories of persons covered by the INA can request cancellation of their PR pursuant to subpart 4 of part B of title VII, which includes aliens whose claims were denied on any of the following grounds: naturalization, adjustment of status, citizenship, entry into the United States as a result of fraud, admission to the U.S., or removal from the country due to national security concerns. For more information regarding these statutory requirements see “Inadmissibility Review” below.\n",
      "##### Answer #1: It takes six months to review all applications filed with INS within the time period set forth by USCIS for adjudication before determining whether an additional administrative proceeding should be initiated against you. If your case is closed prior to completing the initial\n",
      "No, you cannot apply to be employed or receive wages as a worker in the United States until your visa is issued and approved by U.S. Citizenship and Immigration Services (USCIS). Once USCIS issues a green card application with employment authorization (green card) status for you, you will have the option of working legally at that time. If you choose not to use the legal route to become a lawful permanent resident (LPR), you may still obtain temporary nonimmigrant visas from USCIS when they expire so long as those non-immigrant visas remain valid. After obtaining one year’s worth of nonimmigrant visitor permission through a special “visa waiver” program, if you would like to begin employment in the US on an H1B basis after receiving your LPR approval, you must wait 90 days before applying for another entry permit under the same category of visitor permit.\n",
      "#### Information: Please note: This page provides general guidelines regarding immigration law. Specific questions should always be referred to specific offices within USCIS' Office of International Affairs. The answer provided here does not constitute official guidance; it only represents our best interpretation of current regulations and case laws. For more information, please refer to specific cases listed below, which include all forms of labor migration into the country.\n"
     ]
    }
   ],
   "source": [
    "print(query_model(\"What documents do I need for a green card application?\"))\n",
    "print(query_model(\"How long does it take to process an asylum application?\"))\n",
    "print(query_model(\"Can I work while waiting for my visa?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Found 28 JSON files\n",
      "Loaded 425 clean question-answer pairs\n",
      "Dataset shape: (425, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  fter one year, how do I demonstrate that the n...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 340, Validation size: 42, Test size: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 340/340 [00:00<00:00, 65677.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 42/42 [00:00<00:00, 10628.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 4935.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset_clean\n",
      "Loading tokenizer for facebook/opt-1.3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 1735.76 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 1554.23 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 630.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 340\n",
      "Processed validation dataset size: 42\n",
      "Processed test dataset size: 43\n",
      "Preparing model for training...\n",
      "trainable params: 12,582,912 || all params: 724,361,216 || trainable%: 1.7371045994820353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 2111.53 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 4109.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 3/150 [00:40<33:23, 13.63s/it]\n",
      "  2%|▏         | 3/150 [00:52<33:23, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8352134227752686, 'eval_runtime': 1.6538, 'eval_samples_per_second': 25.395, 'eval_steps_per_second': 1.209, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  5%|▍         | 7/150 [01:40<33:55, 14.24s/it]\n",
      "  5%|▍         | 7/150 [01:46<33:55, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.737858772277832, 'eval_runtime': 1.7435, 'eval_samples_per_second': 24.09, 'eval_steps_per_second': 1.147, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 10/150 [02:23<33:01, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7609, 'learning_rate': 1.9990212265199738e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/150 [02:37<32:07, 13.87s/it]\n",
      "  7%|▋         | 11/150 [02:39<32:07, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5971450805664062, 'eval_runtime': 1.5897, 'eval_samples_per_second': 26.419, 'eval_steps_per_second': 1.258, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 10%|█         | 15/150 [03:32<30:38, 13.62s/it]\n",
      " 10%|█         | 15/150 [03:34<30:38, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.477029323577881, 'eval_runtime': 1.5486, 'eval_samples_per_second': 27.121, 'eval_steps_per_second': 1.291, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 18/150 [04:16<31:03, 14.12s/it]\n",
      " 12%|█▏        | 18/150 [04:28<31:03, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.382483720779419, 'eval_runtime': 1.6693, 'eval_samples_per_second': 25.161, 'eval_steps_per_second': 1.198, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 20/150 [04:49<32:48, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5594, 'learning_rate': 1.96496491452281e-05, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/150 [05:17<31:06, 14.58s/it]\n",
      " 15%|█▍        | 22/150 [05:22<31:06, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2463221549987793, 'eval_runtime': 1.569, 'eval_samples_per_second': 26.769, 'eval_steps_per_second': 1.275, 'epoch': 5.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 26/150 [06:13<28:34, 13.83s/it]\n",
      " 17%|█▋        | 26/150 [06:15<28:34, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1070830821990967, 'eval_runtime': 1.4457, 'eval_samples_per_second': 29.051, 'eval_steps_per_second': 1.383, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 20%|██        | 30/150 [07:07<26:38, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3364, 'learning_rate': 1.883869132745561e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 30/150 [07:09<26:38, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.965039610862732, 'eval_runtime': 1.5724, 'eval_samples_per_second': 26.711, 'eval_steps_per_second': 1.272, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 33/150 [07:50<27:01, 13.86s/it]\n",
      " 22%|██▏       | 33/150 [08:01<27:01, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8684237003326416, 'eval_runtime': 1.5431, 'eval_samples_per_second': 27.217, 'eval_steps_per_second': 1.296, 'epoch': 8.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 25%|██▍       | 37/150 [08:50<27:02, 14.36s/it]\n",
      " 25%|██▍       | 37/150 [08:55<27:02, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7902131080627441, 'eval_runtime': 1.5421, 'eval_samples_per_second': 27.236, 'eval_steps_per_second': 1.297, 'epoch': 9.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 40/150 [09:32<25:50, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1243, 'learning_rate': 1.759687084583285e-05, 'epoch': 10.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 41/150 [09:45<24:59, 13.76s/it]\n",
      " 27%|██▋       | 41/150 [09:48<24:59, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7352055311203003, 'eval_runtime': 1.9197, 'eval_samples_per_second': 21.878, 'eval_steps_per_second': 1.042, 'epoch': 10.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 30%|███       | 45/150 [10:40<23:27, 13.41s/it]\n",
      " 30%|███       | 45/150 [10:42<23:27, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.697068691253662, 'eval_runtime': 1.5776, 'eval_samples_per_second': 26.623, 'eval_steps_per_second': 1.268, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 48/150 [11:24<23:45, 13.98s/it]\n",
      " 32%|███▏      | 48/150 [11:36<23:45, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6912609338760376, 'eval_runtime': 1.5924, 'eval_samples_per_second': 26.376, 'eval_steps_per_second': 1.256, 'epoch': 12.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 50/150 [11:57<25:08, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9901, 'learning_rate': 1.5984723141740578e-05, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 52/150 [12:23<23:03, 14.11s/it]\n",
      " 35%|███▍      | 52/150 [12:29<23:03, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6894094944000244, 'eval_runtime': 1.6534, 'eval_samples_per_second': 25.402, 'eval_steps_per_second': 1.21, 'epoch': 13.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 56/150 [13:19<21:30, 13.72s/it]\n",
      " 37%|███▋      | 56/150 [13:22<21:30, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6972147226333618, 'eval_runtime': 1.6674, 'eval_samples_per_second': 25.189, 'eval_steps_per_second': 1.199, 'epoch': 14.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 40%|████      | 60/150 [14:14<20:07, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9258, 'learning_rate': 1.408083612243465e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 60/150 [14:16<20:07, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.699666142463684, 'eval_runtime': 1.5924, 'eval_samples_per_second': 26.375, 'eval_steps_per_second': 1.256, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 63/150 [14:57<20:02, 13.82s/it]\n",
      " 42%|████▏     | 63/150 [15:09<20:02, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6935288906097412, 'eval_runtime': 1.5889, 'eval_samples_per_second': 26.433, 'eval_steps_per_second': 1.259, 'epoch': 16.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 45%|████▍     | 67/150 [15:58<20:00, 14.46s/it]\n",
      " 45%|████▍     | 67/150 [16:03<20:00, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6900336742401123, 'eval_runtime': 1.6039, 'eval_samples_per_second': 26.186, 'eval_steps_per_second': 1.247, 'epoch': 17.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 70/150 [16:40<18:50, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9248, 'learning_rate': 1.1978019209855174e-05, 'epoch': 18.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 71/150 [16:53<18:06, 13.75s/it]\n",
      " 47%|████▋     | 71/150 [16:56<18:06, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6903163194656372, 'eval_runtime': 1.6168, 'eval_samples_per_second': 25.978, 'eval_steps_per_second': 1.237, 'epoch': 18.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 75/150 [17:48<16:42, 13.37s/it]\n",
      " 50%|█████     | 75/150 [17:50<16:42, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6927034854888916, 'eval_runtime': 1.9952, 'eval_samples_per_second': 21.051, 'eval_steps_per_second': 1.002, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 78/150 [18:31<16:44, 13.95s/it]\n",
      " 52%|█████▏    | 78/150 [18:43<16:44, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6937075853347778, 'eval_runtime': 1.6165, 'eval_samples_per_second': 25.982, 'eval_steps_per_second': 1.237, 'epoch': 20.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 80/150 [19:04<17:23, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8703, 'learning_rate': 9.778779128468133e-06, 'epoch': 21.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 82/150 [19:32<16:11, 14.29s/it]\n",
      " 55%|█████▍    | 82/150 [19:37<16:11, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6938203573226929, 'eval_runtime': 1.8243, 'eval_samples_per_second': 23.022, 'eval_steps_per_second': 1.096, 'epoch': 21.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 86/150 [20:27<14:34, 13.67s/it]\n",
      " 57%|█████▋    | 86/150 [20:29<14:34, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6933379173278809, 'eval_runtime': 1.559, 'eval_samples_per_second': 26.94, 'eval_steps_per_second': 1.283, 'epoch': 22.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 90/150 [21:20<13:02, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8667, 'learning_rate': 7.590322975433857e-06, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 90/150 [21:22<13:02, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6936701536178589, 'eval_runtime': 1.5707, 'eval_samples_per_second': 26.739, 'eval_steps_per_second': 1.273, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 93/150 [22:03<12:57, 13.64s/it]\n",
      " 62%|██████▏   | 93/150 [22:14<12:57, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6948999166488647, 'eval_runtime': 1.5508, 'eval_samples_per_second': 27.083, 'eval_steps_per_second': 1.29, 'epoch': 24.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 65%|██████▍   | 97/150 [23:02<12:31, 14.18s/it]\n",
      " 65%|██████▍   | 97/150 [23:07<12:31, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.693650722503662, 'eval_runtime': 1.8207, 'eval_samples_per_second': 23.067, 'eval_steps_per_second': 1.098, 'epoch': 25.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 100/150 [23:45<11:42, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.873, 'learning_rate': 5.519332160124215e-06, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 101/150 [23:58<11:16, 13.81s/it]\n",
      " 67%|██████▋   | 101/150 [24:01<11:16, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6925392150878906, 'eval_runtime': 1.5616, 'eval_samples_per_second': 26.896, 'eval_steps_per_second': 1.281, 'epoch': 26.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 105/150 [24:53<10:05, 13.46s/it]\n",
      " 70%|███████   | 105/150 [24:55<10:05, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.692589521408081, 'eval_runtime': 1.5345, 'eval_samples_per_second': 27.371, 'eval_steps_per_second': 1.303, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 108/150 [25:36<09:44, 13.93s/it]\n",
      " 72%|███████▏  | 108/150 [25:47<09:44, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.692967414855957, 'eval_runtime': 1.5507, 'eval_samples_per_second': 27.084, 'eval_steps_per_second': 1.29, 'epoch': 28.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 110/150 [26:08<09:51, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8477, 'learning_rate': 3.6667619695195287e-06, 'epoch': 29.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 112/150 [26:35<09:03, 14.31s/it]\n",
      " 75%|███████▍  | 112/150 [26:41<09:03, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6944692134857178, 'eval_runtime': 1.5757, 'eval_samples_per_second': 26.654, 'eval_steps_per_second': 1.269, 'epoch': 29.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 116/150 [27:31<07:47, 13.76s/it]\n",
      " 77%|███████▋  | 116/150 [27:34<07:47, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6957935094833374, 'eval_runtime': 1.6097, 'eval_samples_per_second': 26.092, 'eval_steps_per_second': 1.242, 'epoch': 30.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 120/150 [28:25<06:40, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8524, 'learning_rate': 2.1229202668228197e-06, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 120/150 [28:27<06:40, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6959683895111084, 'eval_runtime': 1.5518, 'eval_samples_per_second': 27.066, 'eval_steps_per_second': 1.289, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 123/150 [29:08<06:11, 13.75s/it]\n",
      " 82%|████████▏ | 123/150 [29:20<06:11, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6955112218856812, 'eval_runtime': 1.6069, 'eval_samples_per_second': 26.137, 'eval_steps_per_second': 1.245, 'epoch': 32.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 85%|████████▍ | 127/150 [30:09<05:32, 14.45s/it]\n",
      " 85%|████████▍ | 127/150 [30:13<05:32, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6950746774673462, 'eval_runtime': 1.5285, 'eval_samples_per_second': 27.478, 'eval_steps_per_second': 1.308, 'epoch': 33.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 130/150 [30:51<04:44, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8739, 'learning_rate': 9.630652236279626e-07, 'epoch': 34.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 131/150 [31:04<04:19, 13.68s/it]\n",
      " 87%|████████▋ | 131/150 [31:07<04:19, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6946264505386353, 'eval_runtime': 1.8569, 'eval_samples_per_second': 22.619, 'eval_steps_per_second': 1.077, 'epoch': 34.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 135/150 [31:58<03:20, 13.36s/it]\n",
      " 90%|█████████ | 135/150 [32:00<03:20, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6945058107376099, 'eval_runtime': 1.572, 'eval_samples_per_second': 26.718, 'eval_steps_per_second': 1.272, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 138/150 [32:41<02:45, 13.83s/it]\n",
      " 92%|█████████▏| 138/150 [32:53<02:45, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6944468021392822, 'eval_runtime': 1.8255, 'eval_samples_per_second': 23.008, 'eval_steps_per_second': 1.096, 'epoch': 36.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 140/150 [33:14<02:28, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8417, 'learning_rate': 2.4373668447493225e-07, 'epoch': 37.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 142/150 [33:42<01:55, 14.38s/it]\n",
      " 95%|█████████▍| 142/150 [33:47<01:55, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6944329738616943, 'eval_runtime': 1.5759, 'eval_samples_per_second': 26.651, 'eval_steps_per_second': 1.269, 'epoch': 37.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 146/150 [34:37<00:54, 13.64s/it]\n",
      " 97%|█████████▋| 146/150 [34:40<00:54, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6945277452468872, 'eval_runtime': 1.7325, 'eval_samples_per_second': 24.242, 'eval_steps_per_second': 1.154, 'epoch': 38.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 150/150 [35:32<00:00, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8467, 'learning_rate': 0.0, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [35:33<00:00, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6945019960403442, 'eval_runtime': 1.5686, 'eval_samples_per_second': 26.776, 'eval_steps_per_second': 1.275, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [35:34<00:00, 14.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2134.0319, 'train_samples_per_second': 7.966, 'train_steps_per_second': 0.07, 'train_loss': 2.0329349136352537, 'epoch': 40.0}\n",
      "Model saved to ./immigration_assistant_final\n",
      "Testing model on examples...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses from base and fine-tuned models...\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: If you have not been notified that your application for asylum will be considered, contact our Asylum Assistance Center at 212-621-7000 or email [email protected] with all details regarding your claim and request information on how to file it online by May 25, 2018. Once we receive the requested documents and other supporting documentation in response to your NOID, we may begin processing your petition within 30 days of receipt, but if we find any deficiencies during the process, we must take additional time before proceeding further as required under section 208(a)(2), paragraph 2(b). Please note that there is no deadline for filing your petitions; however, we cannot grant your requests until we complete the review process and determine whether you meet the requirements set forth above. We encourage you to submit evidence related to your case early so that you don’t wait too long between submitting your Form 483 and receiving notice of intent. The sooner you submit your request for consideration, the better. You should also keep track of when each stage of the appeal begins – including where we ask questions such as “Do you live in Mexico today?” and “Have you ever lived outside of the United States?“ – because we typically reach\n",
      "Fine-tuned Model Output: If you were notified that your application for asylum was denied, but did not receive notice by letter or email, please contact our Asylum Support Center at 877-823-4357 or via e-mail at support@asylumhelpusa.org and we will be able provide you with instructions on how to file a petition for review in order to reopen your case. Please note that if there is no notification by letter nor by electronic mail, it may take up to 30 days before you get any further instruction on filing a Petition for Review. For more details on why an applicant might experience these delays, read “How Long Does It Take To File A Petition for Reopening Of An Application For Asylum In The U.S.?” below. Once you have filed your petition for reopening of an asylum claim, you should expect additional time until the Government has had opportunity to consider all arguments presented during the hearing process – which could include several months after the date on the NOID. We recommend contacting us within 90 calendar days after receipt of your petition so that we may begin processing your request as soon as possible. After submitting your request through one of our approved channels, including by calling 1-877-551-2572, you\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: If you meet the eligibility requirements, your state may issue you with a driver’s license (or other appropriate identification) without any required documentation and as long as it is not a felony conviction or if you have been convicted of no more than one traffic offense in the last 10 years. Your DMV can provide further instructions on how to obtain such licenses at www.dmv.ca.gov/get-a-driver-license.\n",
      "For more questions regarding drivers' education requirements please contact the California Department of Education: 888-821-7241 x2; dcdesignations@cde.caad.org ; https://wwwemic.education/dccf/. For additional information concerning the process by which undocumented students who graduate from high school will be able to get their learner's permit after graduation visit our website at http://emicinfo.ed.ucop.edu/about_us/immigrant/immigration_programs/newstudents.cfm. The process involves submitting an application through EDUCARE Online Form A (form D). After receiving all necessary documents and completing the online application form, applicants must attend the following appointment times and dates: Monday - Friday 7am - 5\n",
      "Fine-tuned Model Output: Yes, you may be entitled to apply for and receive a driver’s license if your state has not prohibited the practice of granting licenses or permits by individuals who do not meet certain legal requirements as set forth in section 5-3(c)(1) (§ 6103b). To qualify under this requirement, applicants must have completed their basic education at least two years prior to applying; possess a valid Social Security number from the date of birth on file with USCIS; provide evidence that they reside within the United States during application submission; show proof of citizenship through an official document issued by the U.S. Department of State (such as a passport); prove age eligibility based upon birth certificate records showing a birthdate between January 1, 1949 and December 31, 1973 inclusive.; and demonstrate satisfactory academic performance sufficient to earn a bachelor's degree or higher equivalent before graduation in order to satisfy the minimum educational requirement outlined above. Applicants can also submit documentation to establish their proficiency in English language skills necessary for successful completion of the driver training program. We will consider whether we believe the applicant is competent enough to drive safely and responsibly when considering all aspects of their application including documents submitted via mail, faxes, or online services such as eVerify.\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A social security number is required for many types of employment, including in the private sector where employers may require that applicants have one as part of their eligibility to work with them or hold certain government positions. The IRS also requires individuals who seek U-1 status (nonimmigrant visa) under section 212(a)(3) to submit proof of citizenship via Form I-485, Evidence of Citizenship; however, some foreign nationals do not necessarily file evidence of citizenship if they meet the requirements laid out below. For example, individuals whose naturalization papers contain no documentation indicating that they were born outside of the United States but obtained such documents within 10 years of age may still be eligible for a green card without filing proof of nationality through Form I‑485 unless they satisfy all other requirements specified above. If you believe your application was denied because it did not include proof of birth abroad, we encourage you to send us a copy of your original documentation so our review process will continue once submitted again, which should take less than 90 days after submission. We cannot provide legal advice on whether an individual’s application has been rejected based solely on lack of proof of national origin due to failure to attach a valid document proving identity. Please contact us at 781-9\n",
      "Fine-tuned Model Output: A nonimmigrant is required to submit proof of identity, address, date of birth and citizenship documents for the purpose of obtaining his or her social security number (SSN). Proof of identification may include a copy of one’s passport; driver's license/state ID cards with your name on them; utility bills showing you have lived at home in the United States for many years without living abroad permanently (or if there has been any time period during which you did not live outside of the U.S., including periods when you were under 18); school records listing all grades attended since kindergarten through 12th grade; bank statements from several banks where you deposit money regularly; government-issued tax forms such as W4 form(s) showing that you pay taxes every year in full; bank statement showing payments made by a business account owned by you and others using it; financial transaction records related to income earned over several months so far in 2019, including those involving foreign sources; employment contracts signed by employers who hired you directly rather than hiring you indirectly via an agency like H1B visa sponsorships; copies of letters of recommendation received from work experience sites, college admissions offices, internship opportunities, etc.; and any other documentation that shows you meet the eligibility\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "Base Model:\n",
      "{'rouge1': np.float64(0.21396819080418167), 'rouge2': np.float64(0.020623822953614146), 'rougeL': np.float64(0.10680132534346824), 'rougeLsum': np.float64(0.1048218297172967)}\n",
      "\n",
      "Fine-tuned Model:\n",
      "{'rouge1': np.float64(0.20909799470447354), 'rouge2': np.float64(0.015369339518773912), 'rougeL': np.float64(0.096839727243074), 'rougeLsum': np.float64(0.09695575464852801)}\n",
      "\n",
      "Saved comparison results to ./model_comparison_results.csv\n",
      "\n",
      "Training and evaluation complete!\n",
      "\n",
      "You can now use the query_model() function to test your model interactively.\n",
      "Example: response = query_model('What is the processing time for a green card application?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_final\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final\"\n",
    "RESULTS_CSV = \"./model_comparison_results.csv\"\n",
    "\n",
    "# Open-access model that doesn't require authentication\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # 1.3B parameters, open access\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"Loading and cleaning data...\")\n",
    "all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the clean dataset to disk\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "dataset_dict.save_to_disk(DATASET_PATH)\n",
    "print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Load Model and Tokenizer\n",
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing function for clean instruction format\n",
    "def preprocess_function(examples):\n",
    "    # Use a clear instruction format without complex templates\n",
    "    formatted_prompts = [\n",
    "        f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {q}\\n\\n### Response:\" \n",
    "        for q in examples[\"Question\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            formatted_prompts,\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,  # Reduced dropout for better learning\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target projection layers in OPT model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    # Add the following to prevent repetition during training\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator for language model training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINAL_MODEL_PATH,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on examples from test set\n",
    "test_questions = test_df['Question'][:10].tolist()  # Test on 10 examples\n",
    "test_answers = test_df['Answer'][:10].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompt for the model\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    \n",
    "    # Base model generation\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,  # Increased repetition penalty\n",
    "        no_repeat_ngram_size=3   # Prevent repeating 3-grams\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the base model output\n",
    "    base_text = base_text.replace(prompt, \"\").strip()\n",
    "    base_text = post_process_response(base_text)\n",
    "    base_model_outputs.append(base_text)\n",
    "    \n",
    "    # Fine-tuned model generation\n",
    "    ft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the fine-tuned model output\n",
    "    ft_text = ft_text.replace(prompt, \"\").strip()\n",
    "    ft_text = post_process_response(ft_text)\n",
    "    peft_model_outputs.append(ft_text)\n",
    "\n",
    "# Print results for a few examples\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions[:3], test_answers[:3], base_model_outputs[:3], peft_model_outputs[:3])):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers[:len(base_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers[:len(peft_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)\n",
    "\n",
    "# Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers[:len(test_questions)],\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nSaved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "# Create a simple inference function to test the model interactively\n",
    "def query_model(question, model=peft_model):\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 300, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Clean the response\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    response = post_process_response(response)\n",
    "    return response\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "print(\"Example: response = query_model('What is the processing time for a green card application?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Found 28 JSON files\n",
      "Loaded 425 clean question-answer pairs\n",
      "Dataset shape: (425, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  fter one year, how do I demonstrate that the n...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 340, Validation size: 42, Test size: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 340/340 [00:00<00:00, 82317.21 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 42/42 [00:00<00:00, 5430.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 8869.19 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset_clean\n",
      "Loading tokenizer for facebook/opt-1.3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 1770.37 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 2396.94 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 570.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 340\n",
      "Processed validation dataset size: 42\n",
      "Processed test dataset size: 43\n",
      "Preparing model for training...\n",
      "trainable params: 12,582,912 || all params: 724,361,216 || trainable%: 1.7371045994820353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 2279.22 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 5799.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  1%|          | 2/200 [00:34<57:24, 17.40s/it]\n",
      "  1%|          | 2/200 [00:51<57:24, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.847487688064575, 'eval_runtime': 1.8094, 'eval_samples_per_second': 23.212, 'eval_steps_per_second': 1.105, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  2%|▎         | 5/200 [01:36<1:02:20, 19.18s/it]\n",
      "  2%|▎         | 5/200 [01:43<1:02:20, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.803516149520874, 'eval_runtime': 1.8157, 'eval_samples_per_second': 23.132, 'eval_steps_per_second': 1.102, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [02:31<58:42, 18.35s/it]  \n",
      "  4%|▍         | 8/200 [02:34<58:42, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.725045680999756, 'eval_runtime': 1.8514, 'eval_samples_per_second': 22.685, 'eval_steps_per_second': 1.08, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [03:08<58:09, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7629, 'learning_rate': 2e-05, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [03:24<55:43, 17.69s/it]\n",
      "  6%|▌         | 11/200 [03:26<55:43, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.621805191040039, 'eval_runtime': 1.6517, 'eval_samples_per_second': 25.429, 'eval_steps_per_second': 1.211, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  6%|▋         | 13/200 [04:02<56:40, 18.19s/it]\n",
      "  6%|▋         | 13/200 [04:17<56:40, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5585107803344727, 'eval_runtime': 1.6885, 'eval_samples_per_second': 24.874, 'eval_steps_per_second': 1.184, 'epoch': 4.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 16/200 [05:03<58:12, 18.98s/it]  \n",
      "  8%|▊         | 16/200 [05:09<58:12, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4672675132751465, 'eval_runtime': 2.0909, 'eval_samples_per_second': 20.087, 'eval_steps_per_second': 0.957, 'epoch': 5.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 10%|▉         | 19/200 [05:57<54:56, 18.21s/it]\n",
      " 10%|▉         | 19/200 [06:01<54:56, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.370513916015625, 'eval_runtime': 1.881, 'eval_samples_per_second': 22.328, 'eval_steps_per_second': 1.063, 'epoch': 6.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [06:17<55:51, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5794, 'learning_rate': 1.9863613034027224e-05, 'epoch': 7.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [06:50<52:16, 17.62s/it]\n",
      " 11%|█         | 22/200 [06:52<52:16, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2617435455322266, 'eval_runtime': 1.8213, 'eval_samples_per_second': 23.061, 'eval_steps_per_second': 1.098, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 24/200 [07:29<53:37, 18.28s/it]\n",
      " 12%|█▏        | 24/200 [07:44<53:37, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1968066692352295, 'eval_runtime': 1.7744, 'eval_samples_per_second': 23.671, 'eval_steps_per_second': 1.127, 'epoch': 8.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 14%|█▎        | 27/200 [08:30<55:04, 19.10s/it]\n",
      " 14%|█▎        | 27/200 [08:35<55:04, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0851800441741943, 'eval_runtime': 1.6468, 'eval_samples_per_second': 25.505, 'eval_steps_per_second': 1.215, 'epoch': 9.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 30/200 [09:24<51:56, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.348, 'learning_rate': 1.9458172417006347e-05, 'epoch': 10.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 30/200 [09:27<51:56, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9774829149246216, 'eval_runtime': 1.8293, 'eval_samples_per_second': 22.96, 'eval_steps_per_second': 1.093, 'epoch': 10.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 16%|█▋        | 33/200 [10:17<49:11, 17.67s/it]\n",
      " 16%|█▋        | 33/200 [10:20<49:11, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8714172840118408, 'eval_runtime': 2.0183, 'eval_samples_per_second': 20.809, 'eval_steps_per_second': 0.991, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 35/200 [10:56<50:21, 18.31s/it]\n",
      " 18%|█▊        | 35/200 [11:12<50:21, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.822710633277893, 'eval_runtime': 1.8142, 'eval_samples_per_second': 23.151, 'eval_steps_per_second': 1.102, 'epoch': 12.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [11:57<51:48, 19.19s/it]\n",
      " 19%|█▉        | 38/200 [12:03<51:48, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7714369297027588, 'eval_runtime': 1.7912, 'eval_samples_per_second': 23.447, 'eval_steps_per_second': 1.117, 'epoch': 13.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [12:34<50:18, 18.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1278, 'learning_rate': 1.879473751206489e-05, 'epoch': 14.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [12:52<48:40, 18.37s/it]\n",
      " 20%|██        | 41/200 [12:55<48:40, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7284619808197021, 'eval_runtime': 1.821, 'eval_samples_per_second': 23.064, 'eval_steps_per_second': 1.098, 'epoch': 14.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 44/200 [13:45<45:53, 17.65s/it]\n",
      " 22%|██▏       | 44/200 [13:47<45:53, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6999809741973877, 'eval_runtime': 1.8031, 'eval_samples_per_second': 23.293, 'eval_steps_per_second': 1.109, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [14:23<46:55, 18.28s/it]\n",
      " 23%|██▎       | 46/200 [14:38<46:55, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6949964761734009, 'eval_runtime': 1.8337, 'eval_samples_per_second': 22.904, 'eval_steps_per_second': 1.091, 'epoch': 16.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 49/200 [15:24<48:12, 19.15s/it]\n",
      " 24%|██▍       | 49/200 [15:31<48:12, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6941070556640625, 'eval_runtime': 1.9789, 'eval_samples_per_second': 21.224, 'eval_steps_per_second': 1.011, 'epoch': 17.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [15:44<48:31, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9868, 'learning_rate': 1.789140509396394e-05, 'epoch': 18.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52/200 [16:19<45:11, 18.32s/it]\n",
      " 26%|██▌       | 52/200 [16:22<45:11, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7021595239639282, 'eval_runtime': 1.8205, 'eval_samples_per_second': 23.07, 'eval_steps_per_second': 1.099, 'epoch': 18.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 55/200 [17:12<42:54, 17.76s/it]\n",
      " 28%|██▊       | 55/200 [17:14<42:54, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7045756578445435, 'eval_runtime': 1.8095, 'eval_samples_per_second': 23.21, 'eval_steps_per_second': 1.105, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 57/200 [17:51<43:48, 18.38s/it]\n",
      " 28%|██▊       | 57/200 [18:06<43:48, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.698886513710022, 'eval_runtime': 1.8343, 'eval_samples_per_second': 22.898, 'eval_steps_per_second': 1.09, 'epoch': 20.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [18:51<44:20, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9386, 'learning_rate': 1.6772815716257414e-05, 'epoch': 21.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 60/200 [18:58<44:20, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6958081722259521, 'eval_runtime': 1.832, 'eval_samples_per_second': 22.926, 'eval_steps_per_second': 1.092, 'epoch': 21.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 63/200 [19:46<41:27, 18.16s/it]\n",
      " 32%|███▏      | 63/200 [19:49<41:27, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6989738941192627, 'eval_runtime': 2.0615, 'eval_samples_per_second': 20.373, 'eval_steps_per_second': 0.97, 'epoch': 22.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [20:39<39:28, 17.68s/it]\n",
      " 33%|███▎      | 66/200 [20:41<39:28, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.697507381439209, 'eval_runtime': 1.681, 'eval_samples_per_second': 24.986, 'eval_steps_per_second': 1.19, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 68/200 [21:17<39:45, 18.07s/it]\n",
      " 34%|███▍      | 68/200 [21:33<39:45, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6943410634994507, 'eval_runtime': 1.8301, 'eval_samples_per_second': 22.95, 'eval_steps_per_second': 1.093, 'epoch': 24.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 35%|███▌      | 70/200 [22:00<42:07, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9046, 'learning_rate': 1.5469481581224274e-05, 'epoch': 25.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [22:18<41:19, 19.22s/it]\n",
      " 36%|███▌      | 71/200 [22:25<41:19, 19.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6931068897247314, 'eval_runtime': 1.796, 'eval_samples_per_second': 23.386, 'eval_steps_per_second': 1.114, 'epoch': 25.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [23:13<38:31, 18.34s/it]\n",
      " 37%|███▋      | 74/200 [23:16<38:31, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6958011388778687, 'eval_runtime': 1.8568, 'eval_samples_per_second': 22.619, 'eval_steps_per_second': 1.077, 'epoch': 26.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 77/200 [24:06<36:15, 17.69s/it]\n",
      " 38%|███▊      | 77/200 [24:08<36:15, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7006253004074097, 'eval_runtime': 1.835, 'eval_samples_per_second': 22.888, 'eval_steps_per_second': 1.09, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 40%|███▉      | 79/200 [24:44<36:51, 18.27s/it]\n",
      " 40%|███▉      | 79/200 [25:00<36:51, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7014676332473755, 'eval_runtime': 1.9267, 'eval_samples_per_second': 21.799, 'eval_steps_per_second': 1.038, 'epoch': 28.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 40%|████      | 80/200 [25:10<40:44, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8825, 'learning_rate': 1.4016954246529697e-05, 'epoch': 29.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 82/200 [25:45<37:27, 19.05s/it]\n",
      " 41%|████      | 82/200 [25:52<37:27, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6990200281143188, 'eval_runtime': 1.8196, 'eval_samples_per_second': 23.083, 'eval_steps_per_second': 1.099, 'epoch': 29.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 42%|████▎     | 85/200 [26:40<35:12, 18.37s/it]\n",
      " 42%|████▎     | 85/200 [26:43<35:12, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6996970176696777, 'eval_runtime': 1.6385, 'eval_samples_per_second': 25.634, 'eval_steps_per_second': 1.221, 'epoch': 30.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [27:33<32:50, 17.60s/it]\n",
      " 44%|████▍     | 88/200 [27:35<32:50, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7004210948944092, 'eval_runtime': 1.8033, 'eval_samples_per_second': 23.29, 'eval_steps_per_second': 1.109, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [28:10<33:05, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8613, 'learning_rate': 1.2454854871407993e-05, 'epoch': 32.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▌     | 90/200 [28:26<33:05, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7016347646713257, 'eval_runtime': 1.8096, 'eval_samples_per_second': 23.21, 'eval_steps_per_second': 1.105, 'epoch': 32.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 46%|████▋     | 93/200 [29:12<33:58, 19.05s/it]\n",
      " 46%|████▋     | 93/200 [29:18<33:58, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7014573812484741, 'eval_runtime': 1.8299, 'eval_samples_per_second': 22.951, 'eval_steps_per_second': 1.093, 'epoch': 33.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [30:06<31:41, 18.28s/it]\n",
      " 48%|████▊     | 96/200 [30:10<31:41, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.702175498008728, 'eval_runtime': 2.003, 'eval_samples_per_second': 20.969, 'eval_steps_per_second': 0.999, 'epoch': 34.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 99/200 [31:00<29:47, 17.70s/it]\n",
      " 50%|████▉     | 99/200 [31:01<29:47, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7037568092346191, 'eval_runtime': 1.6814, 'eval_samples_per_second': 24.979, 'eval_steps_per_second': 1.189, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 100/200 [31:19<30:22, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8457, 'learning_rate': 1.0825793454723325e-05, 'epoch': 36.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [31:37<29:55, 18.13s/it]\n",
      " 50%|█████     | 101/200 [31:53<29:55, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7039384841918945, 'eval_runtime': 1.7989, 'eval_samples_per_second': 23.347, 'eval_steps_per_second': 1.112, 'epoch': 36.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [32:38<30:28, 19.05s/it]\n",
      " 52%|█████▏    | 104/200 [32:45<30:28, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7037062644958496, 'eval_runtime': 1.789, 'eval_samples_per_second': 23.477, 'eval_steps_per_second': 1.118, 'epoch': 37.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 54%|█████▎    | 107/200 [33:33<28:22, 18.31s/it]\n",
      " 54%|█████▎    | 107/200 [33:37<28:22, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7044517993927002, 'eval_runtime': 1.7454, 'eval_samples_per_second': 24.064, 'eval_steps_per_second': 1.146, 'epoch': 38.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [34:26<26:25, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8333, 'learning_rate': 9.174206545276678e-06, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 110/200 [34:28<26:25, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7064180374145508, 'eval_runtime': 1.8356, 'eval_samples_per_second': 22.881, 'eval_steps_per_second': 1.09, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [35:04<26:41, 18.20s/it]\n",
      " 56%|█████▌    | 112/200 [35:20<26:41, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7077680826187134, 'eval_runtime': 1.9728, 'eval_samples_per_second': 21.289, 'eval_steps_per_second': 1.014, 'epoch': 40.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 57%|█████▊    | 115/200 [36:06<27:04, 19.11s/it]\n",
      " 57%|█████▊    | 115/200 [36:12<27:04, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.708708643913269, 'eval_runtime': 1.8184, 'eval_samples_per_second': 23.097, 'eval_steps_per_second': 1.1, 'epoch': 41.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 118/200 [37:00<25:02, 18.33s/it]\n",
      " 59%|█████▉    | 118/200 [37:04<25:02, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7073900699615479, 'eval_runtime': 1.7608, 'eval_samples_per_second': 23.852, 'eval_steps_per_second': 1.136, 'epoch': 42.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 120/200 [37:38<24:39, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8337, 'learning_rate': 7.545145128592009e-06, 'epoch': 43.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [37:54<23:16, 17.68s/it]\n",
      " 60%|██████    | 121/200 [37:55<23:16, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7064483165740967, 'eval_runtime': 1.8248, 'eval_samples_per_second': 23.016, 'eval_steps_per_second': 1.096, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 123/200 [38:31<23:27, 18.28s/it]\n",
      " 62%|██████▏   | 123/200 [38:47<23:27, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7063566446304321, 'eval_runtime': 1.789, 'eval_samples_per_second': 23.477, 'eval_steps_per_second': 1.118, 'epoch': 44.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [39:32<23:33, 19.10s/it]\n",
      " 63%|██████▎   | 126/200 [39:39<23:33, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7066733837127686, 'eval_runtime': 1.8671, 'eval_samples_per_second': 22.494, 'eval_steps_per_second': 1.071, 'epoch': 45.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 129/200 [40:27<21:41, 18.33s/it]\n",
      " 64%|██████▍   | 129/200 [40:31<21:41, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7075181007385254, 'eval_runtime': 2.0817, 'eval_samples_per_second': 20.176, 'eval_steps_per_second': 0.961, 'epoch': 46.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [40:46<21:42, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8194, 'learning_rate': 5.983045753470308e-06, 'epoch': 47.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 132/200 [41:21<20:07, 17.76s/it]\n",
      " 66%|██████▌   | 132/200 [41:23<20:07, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7084261178970337, 'eval_runtime': 1.8472, 'eval_samples_per_second': 22.738, 'eval_steps_per_second': 1.083, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [41:59<20:07, 18.30s/it]\n",
      " 67%|██████▋   | 134/200 [42:14<20:07, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7094347476959229, 'eval_runtime': 1.8103, 'eval_samples_per_second': 23.2, 'eval_steps_per_second': 1.105, 'epoch': 48.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 137/200 [43:00<20:05, 19.13s/it]\n",
      " 68%|██████▊   | 137/200 [43:06<20:05, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.709874153137207, 'eval_runtime': 1.8356, 'eval_samples_per_second': 22.881, 'eval_steps_per_second': 1.09, 'epoch': 49.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [43:55<18:31, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8099, 'learning_rate': 4.530518418775734e-06, 'epoch': 50.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 140/200 [43:58<18:31, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7104532718658447, 'eval_runtime': 1.814, 'eval_samples_per_second': 23.153, 'eval_steps_per_second': 1.103, 'epoch': 50.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 143/200 [44:49<16:53, 17.77s/it]\n",
      " 72%|███████▏  | 143/200 [44:50<16:53, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7105985879898071, 'eval_runtime': 1.692, 'eval_samples_per_second': 24.822, 'eval_steps_per_second': 1.182, 'epoch': 52.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 72%|███████▎  | 145/200 [45:26<16:40, 18.18s/it]\n",
      " 72%|███████▎  | 145/200 [45:42<16:40, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7105295658111572, 'eval_runtime': 2.0492, 'eval_samples_per_second': 20.496, 'eval_steps_per_second': 0.976, 'epoch': 52.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 148/200 [46:27<16:26, 18.97s/it]\n",
      " 74%|███████▍  | 148/200 [46:33<16:26, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7105573415756226, 'eval_runtime': 1.7892, 'eval_samples_per_second': 23.474, 'eval_steps_per_second': 1.118, 'epoch': 53.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [47:05<15:51, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8121, 'learning_rate': 3.2271842837425917e-06, 'epoch': 54.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 151/200 [47:22<14:58, 18.34s/it]\n",
      " 76%|███████▌  | 151/200 [47:26<14:58, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7104713916778564, 'eval_runtime': 1.8058, 'eval_samples_per_second': 23.258, 'eval_steps_per_second': 1.108, 'epoch': 54.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 154/200 [48:15<13:28, 17.58s/it]\n",
      " 77%|███████▋  | 154/200 [48:17<13:28, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7108784914016724, 'eval_runtime': 1.8397, 'eval_samples_per_second': 22.83, 'eval_steps_per_second': 1.087, 'epoch': 56.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 156/200 [48:53<13:19, 18.17s/it]\n",
      " 78%|███████▊  | 156/200 [49:08<13:19, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.711111307144165, 'eval_runtime': 1.6997, 'eval_samples_per_second': 24.711, 'eval_steps_per_second': 1.177, 'epoch': 56.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 80%|███████▉  | 159/200 [49:54<13:00, 19.03s/it]\n",
      " 80%|███████▉  | 159/200 [50:00<13:00, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7113313674926758, 'eval_runtime': 1.7017, 'eval_samples_per_second': 24.681, 'eval_steps_per_second': 1.175, 'epoch': 57.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 160/200 [50:14<12:53, 19.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7976, 'learning_rate': 2.1085949060360654e-06, 'epoch': 58.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 162/200 [50:49<11:35, 18.32s/it]\n",
      " 81%|████████  | 162/200 [50:52<11:35, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7113996744155884, 'eval_runtime': 1.9334, 'eval_samples_per_second': 21.724, 'eval_steps_per_second': 1.034, 'epoch': 58.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 82%|████████▎ | 165/200 [51:42<10:21, 17.75s/it]\n",
      " 82%|████████▎ | 165/200 [51:44<10:21, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7113734483718872, 'eval_runtime': 1.8222, 'eval_samples_per_second': 23.048, 'eval_steps_per_second': 1.098, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 84%|████████▎ | 167/200 [52:20<09:59, 18.18s/it]\n",
      " 84%|████████▎ | 167/200 [52:36<09:59, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7115631103515625, 'eval_runtime': 1.7625, 'eval_samples_per_second': 23.829, 'eval_steps_per_second': 1.135, 'epoch': 60.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [53:22<09:38, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7975, 'learning_rate': 1.2052624879351105e-06, 'epoch': 61.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▌ | 170/200 [53:28<09:38, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7114768028259277, 'eval_runtime': 1.8068, 'eval_samples_per_second': 23.245, 'eval_steps_per_second': 1.107, 'epoch': 61.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 86%|████████▋ | 173/200 [54:17<08:17, 18.41s/it]\n",
      " 86%|████████▋ | 173/200 [54:20<08:17, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7115662097930908, 'eval_runtime': 1.8128, 'eval_samples_per_second': 23.168, 'eval_steps_per_second': 1.103, 'epoch': 62.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [55:10<07:04, 17.69s/it]\n",
      " 88%|████████▊ | 176/200 [55:12<07:04, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7114976644515991, 'eval_runtime': 1.8216, 'eval_samples_per_second': 23.057, 'eval_steps_per_second': 1.098, 'epoch': 64.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 178/200 [55:48<06:39, 18.17s/it]\n",
      " 89%|████████▉ | 178/200 [56:04<06:39, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7113865613937378, 'eval_runtime': 2.0992, 'eval_samples_per_second': 20.008, 'eval_steps_per_second': 0.953, 'epoch': 64.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 180/200 [56:32<06:36, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8005, 'learning_rate': 5.418275829936537e-07, 'epoch': 65.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 181/200 [56:49<06:03, 19.11s/it]\n",
      " 90%|█████████ | 181/200 [56:55<06:03, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.711418867111206, 'eval_runtime': 1.8233, 'eval_samples_per_second': 23.035, 'eval_steps_per_second': 1.097, 'epoch': 65.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 184/200 [57:44<04:52, 18.31s/it]\n",
      " 92%|█████████▏| 184/200 [57:47<04:52, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7115107774734497, 'eval_runtime': 1.7834, 'eval_samples_per_second': 23.551, 'eval_steps_per_second': 1.121, 'epoch': 66.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 94%|█████████▎| 187/200 [58:37<03:51, 17.80s/it]\n",
      " 94%|█████████▎| 187/200 [58:39<03:51, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7115285396575928, 'eval_runtime': 1.8036, 'eval_samples_per_second': 23.287, 'eval_steps_per_second': 1.109, 'epoch': 68.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 189/200 [59:16<03:22, 18.39s/it]\n",
      " 94%|█████████▍| 189/200 [59:31<03:22, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7113776206970215, 'eval_runtime': 1.826, 'eval_samples_per_second': 23.001, 'eval_steps_per_second': 1.095, 'epoch': 68.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 190/200 [59:41<03:24, 20.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7932, 'learning_rate': 1.3638696597277678e-07, 'epoch': 69.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 192/200 [1:00:16<02:31, 18.99s/it]\n",
      " 96%|█████████▌| 192/200 [1:00:23<02:31, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7114136219024658, 'eval_runtime': 1.8098, 'eval_samples_per_second': 23.206, 'eval_steps_per_second': 1.105, 'epoch': 69.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 195/200 [1:01:11<01:31, 18.31s/it]\n",
      " 98%|█████████▊| 195/200 [1:01:15<01:31, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7114267349243164, 'eval_runtime': 2.0817, 'eval_samples_per_second': 20.176, 'eval_steps_per_second': 0.961, 'epoch': 70.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 198/200 [1:02:05<00:35, 17.81s/it]\n",
      " 99%|█████████▉| 198/200 [1:02:07<00:35, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7114038467407227, 'eval_runtime': 1.8033, 'eval_samples_per_second': 23.29, 'eval_steps_per_second': 1.109, 'epoch': 72.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [1:02:42<00:00, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7955, 'learning_rate': 0.0, 'epoch': 72.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 200/200 [1:02:44<00:00, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7115198373794556, 'eval_runtime': 2.0267, 'eval_samples_per_second': 20.724, 'eval_steps_per_second': 0.987, 'epoch': 72.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [1:02:45<00:00, 18.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3765.2146, 'train_samples_per_second': 9.03, 'train_steps_per_second': 0.053, 'train_loss': 1.9665108394622803, 'epoch': 72.73}\n",
      "Model saved to ./immigration_assistant_final\n",
      "Testing model on examples...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses from base and fine-tuned models...\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: If you receive a NOID, but your USCIS case is still pending, we will not consider it as part of our review process at that time and may continue processing other cases in your group for which there has been no adjudication yet. We cannot give you specific guidance on what type of action you should take if you have a Noid; however, generally speaking, any denial or delay is grounds for re-evaluation by us when your case moves forward with adjudicative proceedings. Please note: If we reject your application because of one of these reasons listed below, you must file an appeal through the Office of Administrative Law within 120 days after receipt of your NOID. To learn more about filing appeals under 8 CFR 212.2(a)(3), please visit our website at www.uscis.gov/appeals. For further questions regarding denials based upon NOIDs, please contact our Customer Service Center at 1-866-821-2345 during business hours Monday – Friday between 9am ET and 5pm PT. If your request was rejected due to nonadjudicative factors such as age, criminal history, domestic violence conviction, previous deportation order, etc., you should expect additional correspondence before submitting another petition unless\n",
      "Fine-tuned Model Output: If you receive a NOID, it is important that you take action promptly. The Department will not accept any responses or submissions after Noid expiration date. To ensure your response is submitted on time and processed effectively, we recommend submitting your comment through our website at www.dhs-reconsideration.com/contact/. Please provide all relevant contact information including your name, address, email address, phone number, fax number, mailing address, and signature for verification purposes only if required by DHS. We encourage you to file your comments in English as well as Spanish with us online using Form 921, Notice of Appeal Requested by USCIS, but also submit them via mail as required under 8 CFR 214(b)(3). This process requires up to 30 days following receipt of a NOI before consideration may begin unless the agency has decided otherwise. In those cases where the agency decides not to consider your appeal, please follow these steps to ensure you have enough evidence to present your case fully when necessary. If you filed a notice of appeal directly with US Citizenship & Immigration Services, which processes appeals without going back to court first, then waitlisted by USCISC, then requested another hearing with USCIS and were rejected once again – here’\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: Yes, you may obtain a driver’s license if you meet certain requirements and have been convicted of a felony or misdemeanor in the past five years (or one year) on your last driving record check; you do not currently have a valid permit to drive from another state or country; and you agree that your current drivers' licenses will expire at least 12 months after completing all required training and passing the required test before obtaining a new license through our system. To learn more about what we consider when evaluating applicants with pending criminal records, visit https://www.dhs-usa.gov/citizen_registration_fees/. If you're interested in learning more about how DACA works, view the FAQ section here. For additional questions regarding whether someone can get a driver's license under DACA, please refer to DHS policy documents available online at www.dsconsumeraccess.com/docs/documents/legal-information/driver-licenses-and-permits#DACA. Note: The rules pertaining to driver licensing differ depending on each individual State's laws as well as federal regulations, including those related to public safety. We encourage you to read these guidelines prior to applying for a CDL while it is still\n",
      "Fine-tuned Model Output: Yes, you may obtain and maintain your driver’s license in the United States if you meet certain requirements under federal law (see “Requirements to Become Registered Drivers Under Federal Law”). We recommend that you consult with your local DMV before applying so they can ensure that it is legal for you to drive on their premises. If you do not have access to one of these locations, we suggest visiting our website at www.dhs-referral.gov/registration or contacting us directly by phone at 1-800-DHS-2ICE (1-844-366-2523) or emailing dsmr@dmv.us. If applicable, please provide proof such as a copy of your U visa receipt, a recent copy of any employment authorization documents issued during your stay in the country, copies of relevant work permits and documentation showing that you were employed while awaiting your application for naturalization. USCIS will review all requests made through its online Form I-765, Application for Naturalized Citizenship. Please note that there is no fee associated with obtaining a valid drivers' license from your state DMV; however, you should keep track of when your driving privileges expire each year to avoid unnecessary fees being charged\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A social security number is required for most U-1 petitioners to process their petitions, including naturalized citizens (i.e., all persons who have become United States nationals). The USCIS requires that the individual submitting Form I-130 meet certain eligibility criteria in order to receive such a number. For example, those under 18 years of age must submit proof of parental support or custody; individuals over 65 years old may request their own cards through Form I‑129B if they live outside of Mexico where there exists no existing alternative means of identification that meets these requirements. In addition, only lawful permanent residents with full employment authorization, but not immigrants without full employment status, may file Form I–131 requesting consideration as a dependent on Form I‐129A while living abroad. Those filing both forms simultaneously should ensure that one contains the appropriate data elements listed below before submission. Individuals whose petition requests additional evidence to determine whether they were born in the United States include birth certificates issued by state health authorities or other government agencies or medical records supporting the petitioner’s claim for citizenship based upon national origin from any source that was available at the time of filing. Also included in the application is documentation proving the parentage or legal relationship between each person involved. Individuals\n",
      "Fine-tuned Model Output: A nonimmigrant who wishes to establish permanent residence in the United States must present documentation that indicates they meet the requirements for such residency, including proof of identity (such as a passport or U.S.-issued driver’s license), employment authorization documents, marriage certificate, birth record, naturalization papers if applicable, and financial accounts with sufficient funds available at all times on request. To establish eligibility for lawful permanent residence, USCIS requires applicants for green cards to submit evidence supporting their ability to pay taxes in each country where they reside. This includes showing how they will be able to provide a stable income once their status is established here. Noncitizens may also file Form I-485 for initial approval after completing necessary legal processes under 8 CFR 2241–2244. The filing fee varies depending upon the type of petition; however, it typically costs $150 per application. Applicants should contact the nearest US Embassy or Consulate for more details regarding fees, process timelines, and other questions related to the Filing Fee Assistance Program. If you do not have access to a local embassy or consulate, you can visit our website to find out what forms we accept online. Once your visa has been approved by the Immigration Service, you may begin traveling abroad without\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "Base Model:\n",
      "{'rouge1': np.float64(0.21372844098790894), 'rouge2': np.float64(0.02715959019578343), 'rougeL': np.float64(0.10559679172976923), 'rougeLsum': np.float64(0.10558255824450026)}\n",
      "\n",
      "Fine-tuned Model:\n",
      "{'rouge1': np.float64(0.21070117782347858), 'rouge2': np.float64(0.022683386436642188), 'rougeL': np.float64(0.10120802712180615), 'rougeLsum': np.float64(0.10149462035221349)}\n",
      "\n",
      "Saved comparison results to ./model_comparison_results.csv\n",
      "\n",
      "Training and evaluation complete!\n",
      "\n",
      "You can now use the query_model() function to test your model interactively.\n",
      "Example: response = query_model('What is the processing time for a green card application?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_final\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final\"\n",
    "RESULTS_CSV = \"./model_comparison_results.csv\"\n",
    "\n",
    "# Open-access model that doesn't require authentication\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # 1.3B parameters, open access\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"Loading and cleaning data...\")\n",
    "all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the clean dataset to disk\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "dataset_dict.save_to_disk(DATASET_PATH)\n",
    "print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Load Model and Tokenizer\n",
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing function for clean instruction format\n",
    "def preprocess_function(examples):\n",
    "    # Use a clear instruction format without complex templates\n",
    "    formatted_prompts = [\n",
    "        f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {q}\\n\\n### Response:\" \n",
    "        for q in examples[\"Question\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            formatted_prompts,\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,  # Reduced dropout for better learning\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target projection layers in OPT model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"].21\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    # Add the following to prevent repetition during training\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator for language model training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINAL_MODEL_PATH,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on examples from test set\n",
    "test_questions = test_df['Question'][:10].tolist()  # Test on 10 examples\n",
    "test_answers = test_df['Answer'][:10].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompt for the model\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    \n",
    "    # Base model generation\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,  # Increased repetition penalty\n",
    "        no_repeat_ngram_size=3   # Prevent repeating 3-grams\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the base model output\n",
    "    base_text = base_text.replace(prompt, \"\").strip()\n",
    "    base_text = post_process_response(base_text)\n",
    "    base_model_outputs.append(base_text)\n",
    "    \n",
    "    # Fine-tuned model generation\n",
    "    ft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the fine-tuned model output\n",
    "    ft_text = ft_text.replace(prompt, \"\").strip()\n",
    "    ft_text = post_process_response(ft_text)\n",
    "    peft_model_outputs.append(ft_text)\n",
    "\n",
    "# Print results for a few examples\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions[:3], test_answers[:3], base_model_outputs[:3], peft_model_outputs[:3])):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers[:len(base_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers[:len(peft_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)\n",
    "\n",
    "# Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers[:len(test_questions)],\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nSaved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "# Create a simple inference function to test the model interactively\n",
    "def query_model(question, model=peft_model):\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 300, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Clean the response\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    response = post_process_response(response)\n",
    "    return response\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "print(\"Example: response = query_model('What is the processing time for a green card application?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_model('What is the processing time for a green card application?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Found 28 JSON files\n",
      "Loaded 425 clean question-answer pairs\n",
      "Dataset shape: (425, 2)\n",
      "Sample data:\n",
      "                                            Question  \\\n",
      "0  fter one year, how do I demonstrate that the n...   \n",
      "1  Where can I find information about vaccination...   \n",
      "\n",
      "                                              Answer  \n",
      "0  International Entrepreneur RuleUnder the Inter...  \n",
      "1  CDC publishes information about vaccinations i...  \n",
      "Train size: 340, Validation size: 42, Test size: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 340/340 [00:00<00:00, 37946.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 42/42 [00:00<00:00, 6009.65 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 43/43 [00:00<00:00, 5365.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./immigration_qa_dataset_clean\n",
      "Loading tokenizer for facebook/opt-1.3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 1562.34 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 1770.64 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 551.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 340\n",
      "Processed validation dataset size: 42\n",
      "Processed test dataset size: 43\n",
      "Preparing model for training...\n",
      "trainable params: 12,582,912 || all params: 724,361,216 || trainable%: 1.7371045994820353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 2155.28 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 5897.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "  1%|          | 3/300 [00:53<1:07:38, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.843508005142212, 'eval_runtime': 1.4971, 'eval_samples_per_second': 28.055, 'eval_steps_per_second': 1.336, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      "  2%|▏         | 7/300 [01:46<1:09:45, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7894909381866455, 'eval_runtime': 1.5816, 'eval_samples_per_second': 26.556, 'eval_steps_per_second': 1.265, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 10/300 [02:24<1:07:48, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7819, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▎         | 11/300 [02:39<1:06:20, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6966001987457275, 'eval_runtime': 1.5994, 'eval_samples_per_second': 26.26, 'eval_steps_per_second': 1.25, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  5%|▌         | 15/300 [03:34<1:04:40, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.579209566116333, 'eval_runtime': 1.5599, 'eval_samples_per_second': 26.925, 'eval_steps_per_second': 1.282, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  6%|▌         | 18/300 [04:28<1:05:54, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4874427318573, 'eval_runtime': 1.7519, 'eval_samples_per_second': 23.973, 'eval_steps_per_second': 1.142, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 20/300 [04:49<1:10:26, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6335, 'learning_rate': 1.9984815164333163e-05, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  7%|▋         | 22/300 [05:22<1:07:17, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3583312034606934, 'eval_runtime': 1.5174, 'eval_samples_per_second': 27.68, 'eval_steps_per_second': 1.318, 'epoch': 5.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  9%|▊         | 26/300 [06:15<1:03:08, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.216672420501709, 'eval_runtime': 1.6099, 'eval_samples_per_second': 26.089, 'eval_steps_per_second': 1.242, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 10%|█         | 30/300 [07:07<1:00:36, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4104, 'learning_rate': 1.9863613034027224e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 30/300 [07:09<1:00:36, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0713632106781006, 'eval_runtime': 1.6171, 'eval_samples_per_second': 25.973, 'eval_steps_per_second': 1.237, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 11%|█         | 33/300 [08:03<1:02:33, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9623329639434814, 'eval_runtime': 1.5517, 'eval_samples_per_second': 27.067, 'eval_steps_per_second': 1.289, 'epoch': 8.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 12%|█▏        | 37/300 [08:56<1:03:24, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8345930576324463, 'eval_runtime': 1.6719, 'eval_samples_per_second': 25.12, 'eval_steps_per_second': 1.196, 'epoch': 9.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 40/300 [09:34<1:01:27, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1742, 'learning_rate': 1.9622680003092503e-05, 'epoch': 10.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 14%|█▎        | 41/300 [09:50<59:43, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7629668712615967, 'eval_runtime': 1.6526, 'eval_samples_per_second': 25.415, 'eval_steps_per_second': 1.21, 'epoch': 10.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 15%|█▌        | 45/300 [10:44<57:08, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7088862657546997, 'eval_runtime': 1.6832, 'eval_samples_per_second': 24.953, 'eval_steps_per_second': 1.188, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 16%|█▌        | 48/300 [11:37<58:44, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6935629844665527, 'eval_runtime': 1.8589, 'eval_samples_per_second': 22.594, 'eval_steps_per_second': 1.076, 'epoch': 12.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 50/300 [11:59<1:02:50, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.006, 'learning_rate': 1.9264940672148018e-05, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 17%|█▋        | 52/300 [12:30<58:14, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6903311014175415, 'eval_runtime': 1.8338, 'eval_samples_per_second': 22.903, 'eval_steps_per_second': 1.091, 'epoch': 13.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 19%|█▊        | 56/300 [13:24<55:46, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6974174976348877, 'eval_runtime': 1.6583, 'eval_samples_per_second': 25.327, 'eval_steps_per_second': 1.206, 'epoch': 14.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 20%|██        | 60/300 [14:16<53:48, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9274, 'learning_rate': 1.879473751206489e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 60/300 [14:18<53:48, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7003488540649414, 'eval_runtime': 1.8852, 'eval_samples_per_second': 22.279, 'eval_steps_per_second': 1.061, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 21%|██        | 63/300 [15:11<54:53, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.693541169166565, 'eval_runtime': 1.7625, 'eval_samples_per_second': 23.83, 'eval_steps_per_second': 1.135, 'epoch': 16.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 22%|██▏       | 67/300 [16:06<56:37, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6909723281860352, 'eval_runtime': 1.6607, 'eval_samples_per_second': 25.29, 'eval_steps_per_second': 1.204, 'epoch': 17.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 70/300 [16:44<54:55, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9174, 'learning_rate': 1.821777815225245e-05, 'epoch': 18.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 24%|██▎       | 71/300 [17:00<53:11, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6928735971450806, 'eval_runtime': 1.6598, 'eval_samples_per_second': 25.304, 'eval_steps_per_second': 1.205, 'epoch': 18.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 25%|██▌       | 75/300 [17:53<50:23, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.695127010345459, 'eval_runtime': 1.6812, 'eval_samples_per_second': 24.983, 'eval_steps_per_second': 1.19, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 26%|██▌       | 78/300 [18:47<51:33, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6963386535644531, 'eval_runtime': 1.5395, 'eval_samples_per_second': 27.282, 'eval_steps_per_second': 1.299, 'epoch': 20.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 80/300 [19:08<54:50, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8595, 'learning_rate': 1.7541066097768965e-05, 'epoch': 21.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 27%|██▋       | 82/300 [19:40<52:18, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.696678876876831, 'eval_runtime': 1.3942, 'eval_samples_per_second': 30.125, 'eval_steps_per_second': 1.435, 'epoch': 21.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 29%|██▊       | 86/300 [20:34<49:11, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6979856491088867, 'eval_runtime': 1.6065, 'eval_samples_per_second': 26.143, 'eval_steps_per_second': 1.245, 'epoch': 22.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 30%|███       | 90/300 [21:25<46:04, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.848, 'learning_rate': 1.6772815716257414e-05, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|███       | 90/300 [21:26<46:04, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7007620334625244, 'eval_runtime': 1.3999, 'eval_samples_per_second': 30.002, 'eval_steps_per_second': 1.429, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 31%|███       | 93/300 [22:20<47:18, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6997026205062866, 'eval_runtime': 1.5671, 'eval_samples_per_second': 26.8, 'eval_steps_per_second': 1.276, 'epoch': 24.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 32%|███▏      | 97/300 [23:13<48:17, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.695892095565796, 'eval_runtime': 1.67, 'eval_samples_per_second': 25.15, 'eval_steps_per_second': 1.198, 'epoch': 25.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 100/300 [23:50<46:56, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8459, 'learning_rate': 1.5922352526649803e-05, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 34%|███▎      | 101/300 [24:06<46:01, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6963555812835693, 'eval_runtime': 1.5542, 'eval_samples_per_second': 27.023, 'eval_steps_per_second': 1.287, 'epoch': 26.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 35%|███▌      | 105/300 [25:00<43:31, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7023065090179443, 'eval_runtime': 1.6271, 'eval_samples_per_second': 25.812, 'eval_steps_per_second': 1.229, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 36%|███▌      | 108/300 [25:52<44:25, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7052775621414185, 'eval_runtime': 1.5309, 'eval_samples_per_second': 27.435, 'eval_steps_per_second': 1.306, 'epoch': 28.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 110/300 [26:13<46:30, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8082, 'learning_rate': 1.5000000000000002e-05, 'epoch': 29.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 37%|███▋      | 112/300 [26:46<44:48, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7082090377807617, 'eval_runtime': 1.6577, 'eval_samples_per_second': 25.337, 'eval_steps_per_second': 1.207, 'epoch': 29.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 39%|███▊      | 116/300 [27:39<42:14, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.707259178161621, 'eval_runtime': 1.8151, 'eval_samples_per_second': 23.139, 'eval_steps_per_second': 1.102, 'epoch': 30.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 40%|████      | 120/300 [28:31<40:20, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8021, 'learning_rate': 1.4016954246529697e-05, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 120/300 [28:33<40:20, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7056208848953247, 'eval_runtime': 1.4186, 'eval_samples_per_second': 29.606, 'eval_steps_per_second': 1.41, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 41%|████      | 123/300 [29:26<40:48, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7048007249832153, 'eval_runtime': 1.5312, 'eval_samples_per_second': 27.429, 'eval_steps_per_second': 1.306, 'epoch': 32.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 42%|████▏     | 127/300 [30:20<41:42, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7089468240737915, 'eval_runtime': 1.8336, 'eval_samples_per_second': 22.906, 'eval_steps_per_second': 1.091, 'epoch': 33.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 130/300 [30:58<40:35, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8131, 'learning_rate': 1.2985148110016947e-05, 'epoch': 34.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 44%|████▎     | 131/300 [31:14<39:03, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7102563381195068, 'eval_runtime': 1.6239, 'eval_samples_per_second': 25.864, 'eval_steps_per_second': 1.232, 'epoch': 34.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 45%|████▌     | 135/300 [32:07<37:03, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7104980945587158, 'eval_runtime': 1.6372, 'eval_samples_per_second': 25.653, 'eval_steps_per_second': 1.222, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 46%|████▌     | 138/300 [33:01<37:47, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7109707593917847, 'eval_runtime': 1.9888, 'eval_samples_per_second': 21.118, 'eval_steps_per_second': 1.006, 'epoch': 36.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 140/300 [33:22<39:56, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7739, 'learning_rate': 1.1917106319237386e-05, 'epoch': 37.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 47%|████▋     | 142/300 [33:55<37:57, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.711273431777954, 'eval_runtime': 1.4598, 'eval_samples_per_second': 28.772, 'eval_steps_per_second': 1.37, 'epoch': 37.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 49%|████▊     | 146/300 [34:48<35:20, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7140793800354004, 'eval_runtime': 1.5134, 'eval_samples_per_second': 27.753, 'eval_steps_per_second': 1.322, 'epoch': 38.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 150/300 [35:40<33:29, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.765, 'learning_rate': 1.0825793454723325e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 150/300 [35:42<33:29, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7137072086334229, 'eval_runtime': 1.6215, 'eval_samples_per_second': 25.903, 'eval_steps_per_second': 1.233, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 51%|█████     | 153/300 [36:36<34:00, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7134851217269897, 'eval_runtime': 1.679, 'eval_samples_per_second': 25.014, 'eval_steps_per_second': 1.191, 'epoch': 40.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 52%|█████▏    | 157/300 [37:30<34:15, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7138193845748901, 'eval_runtime': 1.7637, 'eval_samples_per_second': 23.814, 'eval_steps_per_second': 1.134, 'epoch': 41.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 160/300 [38:07<33:06, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7844, 'learning_rate': 9.724456576318383e-06, 'epoch': 42.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 54%|█████▎    | 161/300 [38:23<32:05, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.71701979637146, 'eval_runtime': 1.8674, 'eval_samples_per_second': 22.491, 'eval_steps_per_second': 1.071, 'epoch': 42.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 55%|█████▌    | 165/300 [39:18<30:22, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7173590660095215, 'eval_runtime': 1.8804, 'eval_samples_per_second': 22.335, 'eval_steps_per_second': 1.064, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 56%|█████▌    | 168/300 [40:11<30:38, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7167714834213257, 'eval_runtime': 1.5351, 'eval_samples_per_second': 27.36, 'eval_steps_per_second': 1.303, 'epoch': 44.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 170/300 [40:32<32:22, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7502, 'learning_rate': 8.626464421815919e-06, 'epoch': 45.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 57%|█████▋    | 172/300 [41:05<30:41, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7175542116165161, 'eval_runtime': 1.9289, 'eval_samples_per_second': 21.774, 'eval_steps_per_second': 1.037, 'epoch': 45.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 59%|█████▊    | 176/300 [41:58<28:23, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7188231945037842, 'eval_runtime': 1.6934, 'eval_samples_per_second': 24.802, 'eval_steps_per_second': 1.181, 'epoch': 46.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 180/300 [42:50<26:52, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7391, 'learning_rate': 7.545145128592009e-06, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 180/300 [42:52<26:52, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7187964916229248, 'eval_runtime': 1.603, 'eval_samples_per_second': 26.201, 'eval_steps_per_second': 1.248, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 61%|██████    | 183/300 [43:45<27:05, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7177810668945312, 'eval_runtime': 1.7277, 'eval_samples_per_second': 24.31, 'eval_steps_per_second': 1.158, 'epoch': 48.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 62%|██████▏   | 187/300 [44:39<26:57, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7189439535140991, 'eval_runtime': 1.6623, 'eval_samples_per_second': 25.266, 'eval_steps_per_second': 1.203, 'epoch': 49.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 190/300 [45:16<25:56, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7425, 'learning_rate': 6.4936244480724575e-06, 'epoch': 50.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 64%|██████▎   | 191/300 [45:33<25:14, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7195024490356445, 'eval_runtime': 1.6168, 'eval_samples_per_second': 25.978, 'eval_steps_per_second': 1.237, 'epoch': 50.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 65%|██████▌   | 195/300 [46:26<23:27, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7202470302581787, 'eval_runtime': 1.6781, 'eval_samples_per_second': 25.028, 'eval_steps_per_second': 1.192, 'epoch': 52.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 66%|██████▌   | 198/300 [47:20<23:46, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.720544695854187, 'eval_runtime': 1.9928, 'eval_samples_per_second': 21.076, 'eval_steps_per_second': 1.004, 'epoch': 52.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 200/300 [47:42<25:12, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7466, 'learning_rate': 5.484666416891109e-06, 'epoch': 53.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 202/300 [48:14<23:31, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7194780111312866, 'eval_runtime': 1.6283, 'eval_samples_per_second': 25.794, 'eval_steps_per_second': 1.228, 'epoch': 53.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 69%|██████▊   | 206/300 [49:08<21:41, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.719663381576538, 'eval_runtime': 1.6571, 'eval_samples_per_second': 25.345, 'eval_steps_per_second': 1.207, 'epoch': 54.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 210/300 [49:59<19:52, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7256, 'learning_rate': 4.530518418775734e-06, 'epoch': 56.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 210/300 [50:01<19:52, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7206614017486572, 'eval_runtime': 1.9562, 'eval_samples_per_second': 21.47, 'eval_steps_per_second': 1.022, 'epoch': 56.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 71%|███████   | 213/300 [50:55<20:18, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7211276292800903, 'eval_runtime': 1.5839, 'eval_samples_per_second': 26.516, 'eval_steps_per_second': 1.263, 'epoch': 56.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 72%|███████▏  | 217/300 [51:49<19:58, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7211543321609497, 'eval_runtime': 1.5551, 'eval_samples_per_second': 27.008, 'eval_steps_per_second': 1.286, 'epoch': 57.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 220/300 [52:27<18:58, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7425, 'learning_rate': 3.6427625179003223e-06, 'epoch': 58.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 74%|███████▎  | 221/300 [52:43<18:08, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7212646007537842, 'eval_runtime': 1.8979, 'eval_samples_per_second': 22.129, 'eval_steps_per_second': 1.054, 'epoch': 58.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 75%|███████▌  | 225/300 [53:37<16:54, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7219167947769165, 'eval_runtime': 1.705, 'eval_samples_per_second': 24.633, 'eval_steps_per_second': 1.173, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 76%|███████▌  | 228/300 [54:31<16:50, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.722355604171753, 'eval_runtime': 1.6446, 'eval_samples_per_second': 25.538, 'eval_steps_per_second': 1.216, 'epoch': 60.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 230/300 [54:52<17:38, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7093, 'learning_rate': 2.8321748683154893e-06, 'epoch': 61.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 77%|███████▋  | 232/300 [55:25<16:19, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7221194505691528, 'eval_runtime': 1.8364, 'eval_samples_per_second': 22.871, 'eval_steps_per_second': 1.089, 'epoch': 61.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 79%|███████▊  | 236/300 [56:18<14:43, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7225074768066406, 'eval_runtime': 1.6275, 'eval_samples_per_second': 25.807, 'eval_steps_per_second': 1.229, 'epoch': 62.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 240/300 [57:11<13:33, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7115, 'learning_rate': 2.1085949060360654e-06, 'epoch': 64.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 240/300 [57:12<13:33, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7230054140090942, 'eval_runtime': 1.6367, 'eval_samples_per_second': 25.661, 'eval_steps_per_second': 1.222, 'epoch': 64.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 81%|████████  | 243/300 [58:06<13:11, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723283290863037, 'eval_runtime': 1.8305, 'eval_samples_per_second': 22.944, 'eval_steps_per_second': 1.093, 'epoch': 64.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 82%|████████▏ | 247/300 [58:59<12:44, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7235742807388306, 'eval_runtime': 1.6351, 'eval_samples_per_second': 25.686, 'eval_steps_per_second': 1.223, 'epoch': 65.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 250/300 [59:37<11:51, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7336, 'learning_rate': 1.4808059116167306e-06, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 84%|████████▎ | 251/300 [59:53<11:22, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7235395908355713, 'eval_runtime': 1.7565, 'eval_samples_per_second': 23.912, 'eval_steps_per_second': 1.139, 'epoch': 66.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 85%|████████▌ | 255/300 [1:00:48<10:08, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723737120628357, 'eval_runtime': 1.8593, 'eval_samples_per_second': 22.589, 'eval_steps_per_second': 1.076, 'epoch': 68.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 86%|████████▌ | 258/300 [1:01:42<09:52, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7239004373550415, 'eval_runtime': 1.5463, 'eval_samples_per_second': 27.162, 'eval_steps_per_second': 1.293, 'epoch': 68.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 260/300 [1:02:03<10:10, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7134, 'learning_rate': 9.564283930242258e-07, 'epoch': 69.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 87%|████████▋ | 262/300 [1:02:35<09:11, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7238432168960571, 'eval_runtime': 1.5629, 'eval_samples_per_second': 26.873, 'eval_steps_per_second': 1.28, 'epoch': 69.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 89%|████████▊ | 266/300 [1:03:29<07:51, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7238762378692627, 'eval_runtime': 1.7855, 'eval_samples_per_second': 23.522, 'eval_steps_per_second': 1.12, 'epoch': 70.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 270/300 [1:04:21<06:44, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7127, 'learning_rate': 5.418275829936537e-07, 'epoch': 72.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 270/300 [1:04:23<06:44, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7238558530807495, 'eval_runtime': 1.5346, 'eval_samples_per_second': 27.369, 'eval_steps_per_second': 1.303, 'epoch': 72.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 91%|█████████ | 273/300 [1:05:16<06:14, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7239043712615967, 'eval_runtime': 1.58, 'eval_samples_per_second': 26.582, 'eval_steps_per_second': 1.266, 'epoch': 72.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 92%|█████████▏| 277/300 [1:06:10<05:30, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7237932682037354, 'eval_runtime': 1.6798, 'eval_samples_per_second': 25.003, 'eval_steps_per_second': 1.191, 'epoch': 73.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 280/300 [1:06:48<04:45, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7384, 'learning_rate': 2.420361737256438e-07, 'epoch': 74.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 94%|█████████▎| 281/300 [1:07:04<04:24, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723901629447937, 'eval_runtime': 1.8386, 'eval_samples_per_second': 22.843, 'eval_steps_per_second': 1.088, 'epoch': 74.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 95%|█████████▌| 285/300 [1:07:58<03:22, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723875880241394, 'eval_runtime': 1.5238, 'eval_samples_per_second': 27.562, 'eval_steps_per_second': 1.312, 'epoch': 76.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 96%|█████████▌| 288/300 [1:08:52<02:48, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723939061164856, 'eval_runtime': 1.6375, 'eval_samples_per_second': 25.649, 'eval_steps_per_second': 1.221, 'epoch': 76.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 290/300 [1:09:13<02:30, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7046, 'learning_rate': 6.069322682050516e-08, 'epoch': 77.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 97%|█████████▋| 292/300 [1:09:46<01:55, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7239011526107788, 'eval_runtime': 1.8006, 'eval_samples_per_second': 23.326, 'eval_steps_per_second': 1.111, 'epoch': 77.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                   \n",
      " 99%|█████████▊| 296/300 [1:10:40<00:55, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7239590883255005, 'eval_runtime': 1.6439, 'eval_samples_per_second': 25.549, 'eval_steps_per_second': 1.217, 'epoch': 78.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 300/300 [1:11:31<00:00, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7099, 'learning_rate': 0.0, 'epoch': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 300/300 [1:11:33<00:00, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.723951816558838, 'eval_runtime': 1.4001, 'eval_samples_per_second': 29.999, 'eval_steps_per_second': 1.429, 'epoch': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [1:11:33<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4293.6793, 'train_samples_per_second': 7.919, 'train_steps_per_second': 0.07, 'train_loss': 1.8776977920532227, 'epoch': 80.0}\n",
      "Model saved to ./immigration_assistant_final\n",
      "Testing model on examples...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses from base and fine-tuned models...\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: If you receive an NOI, it means that your petition has been denied and will not be processed by USCIS for further action until we have reviewed the reasons why our decision was made in error or if we decide to reopen the matter with additional evidence. We may also send out additional notices related to pending cases at any time without prior notice. Please note that there is no guarantee that these notifications will provide us with more information than what appears on the NOIs. For example, many OFA recipients who did not receive a NOI were advised to call 866-817-7243 as soon as possible so they could arrange an appointment before their case was closed permanently. However, even though we advise people to contact them immediately after receiving such an announcement, some OFAs found themselves left stranded when called and told that their case had already been deemed unenforceable and therefore unable to take advantage of their right under law to remain in the United States. In those instances where the notification indicates that someone’s status should continue, however, we strongly recommend contacting the Office of Foreign Assets Control (OFAC), which administers U.S.-based sanctions against foreign governments that violate human rights and international humanitarian laws. The OCA provides\n",
      "Fine-tuned Model Output: If you receive an NOI, it is not automatic that USCIS will deny your application for asylum based on your evidence or lack thereof. An NIO does not automatically mean a decision by DHS to refuse consideration of your request; however, when we consider whether to grant you an entry-based immigrant visa, we must determine if there exists credible fear and reasonable probability of persecution, among other factors. We cannot reject a petition based solely upon our determination regarding the merits of your case without considering additional considerations such as the existence of credible fears and reasonable probabilities of persecution in countries where you may face serious harm should you be returned home. While an NIO is intended to inform us so that we may make informed decisions with regard to your claim, no decision made through an Nio can affect the outcome of any pending petitions involving similar claims for admission into the United States under Title 8, Code of Federal Regulations. In order to protect the integrity of our adjudication process, all cases submitted directly to us via an NRO form need only include one Form I-129F, Application For Asylum Under Section 212(a)(2), of which the principal page should contain both Form I–131A, Evidence of Eligibility, and Forms O-1\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: Yes, you may apply to obtain your driver’s license without fear of being turned away by Immigration and Customs Enforcement (ICE). If you meet the eligibility requirements below, we will issue you with a valid driving license so that you can drive safely in our country. For more details on how we evaluate applicants' eligibility, see Section 821(b)(4) of Title 9, Code of Federal Regulations, which states: \"In addition to consideration under §823c-3(a), if any person meets the following criteria, ICE shall consider whether such individual is a bona fide resident within 180 days after issuance of his or her temporary nonimmigrant visa or other lawful permanent residence: • Is not subject to deportation, removal order, pending deportation, or removal from the United States.\" We encourage you to review our online Form I-765 before applying for a driver�emic card. This form provides detailed instructions on how to submit all required documents including biographical data, address, Social Security number, marriage certificate, proof of age, and documentation of employment. To view this document, visit www.dhs.gov/forms/form-i-765. Once submitted through this website, you cannot change it once you have completed the\n",
      "Fine-tuned Model Output: Yes, you may apply to obtain your driver’s license if you meet the following criteria: At least 18 years of age at time of application or within six months after receipt of your Social Security number and date of birth; Must be able to produce proof that you have been legally admitted into the United States (e.g., U visa); May not possess any illegal substances or weapons on which federal charges could be filed; Has no criminal convictions from foreign jurisdictions or in the U.S.; Is enrolled as full-time undergraduate student(s) studying under the National Endowment for the Arts or other federally funded programs with a grade point average equivalent to 3.0 or higher on their degree or diploma; Have earned sufficient credits during college/university study abroad trips to earn enough credit hours to receive either a bachelor’ s degree or master’ re degree based upon the terms established by the school program where they studied; Have completed all required courses and received certificates of completion prior to applying for licensure; Are willing to pay fees associated with obtaining a valid license; Willing to undergo fingerprint screening tests before receiving a license; Do not intend to use alcohol or tobacco products while driving; Are currently employed without benefits but will be making\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A Nonimmigrant who is not a U.S.-resident alien may apply for the Form I-918, Application to Register with Citizenship or Immigration Services (Form I-765) through USCIS’s eFiling Portal at www.uscis.gov/forms/. If you do not have access to online filing services, please call our Customer Service Center at 1-844-347-3323 between 8 a.m.–5 p.m., Monday–Friday; 9 a. m.–4 p. m., Saturday). The form must be filed in person by mail. Please note that we will accept paper applications if they include all of your required documents, including your original birth certificate. We also require photocopies of supporting documentation such as a marriage license from another country, a copy of both passports, or other travel documents proving where you traveled during your last two years of residence before coming to the United States. For more details on obtaining a social security number, visit the National Institute of Health website here. In addition, when applying for citizenship, a nonimmigrant should submit their fingerprints. To view our list of recommended fingerprinting providers, click here. Once submitted via efilling portal, you will receive instructions\n",
      "Fine-tuned Model Output: A Nonimmigrant may apply for the issuance of a U visa without the requirement to submit a Social Insurance number (Form I-130, Petition for Alien Worker). However, the foreign national must be authorized by USCIS as a bona fide beneficiary under 8 CFR 233.21 through 232.23(b)(2) before he or she may file Form I-129, Application for Visa Waiver Program Approval, on behalf of another person who is not lawfully present in the United States at the time of filing for the application; however, if the alien already has permission from US Citizenship and Immigration Services (USCIS), then it will not be necessary that the petitioner provide the social insurance form prior to filing the petition. This process allows for faster processing times compared with waiting until one year after arrival to request social security numbers when applying directly via Form I131. If the applicant does not have social security authorization yet they should contact their local health department so they can receive a vaccination card issued under 9 CFR 216.31(a)(1); once the individual receives their vaccination cards, we recommend contacting your county’s health department regarding how you get medical records sent electronically. We also encourage you to use our online tool, which provides instructions on\n",
      "\n",
      "--- ROUGE Scores ---\n",
      "Base Model:\n",
      "{'rouge1': np.float64(0.2151947656353111), 'rouge2': np.float64(0.02976205650863438), 'rougeL': np.float64(0.10698586208693855), 'rougeLsum': np.float64(0.10785017641367273)}\n",
      "\n",
      "Fine-tuned Model:\n",
      "{'rouge1': np.float64(0.21441751932210235), 'rouge2': np.float64(0.027579637880574902), 'rougeL': np.float64(0.09830587356972426), 'rougeLsum': np.float64(0.0982690064058751)}\n",
      "\n",
      "Saved comparison results to ./model_comparison_results.csv\n",
      "\n",
      "Training and evaluation complete!\n",
      "\n",
      "You can now use the query_model() function to test your model interactively.\n",
      "Example: response = query_model('What is the processing time for a green card application?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_final\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final\"\n",
    "RESULTS_CSV = \"./model_comparison_results.csv\"\n",
    "\n",
    "# Open-access model that doesn't require authentication\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # 1.3B parameters, open access\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"Loading and cleaning data...\")\n",
    "all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_qa_data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size+val_size]\n",
    "test_df = df[train_size+val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a dataset dictionary\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Save the clean dataset to disk\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "dataset_dict.save_to_disk(DATASET_PATH)\n",
    "print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "# Step 2: Load Model and Tokenizer\n",
    "# Define quantization config for 4-bit precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization config\n",
    "print(\"Loading model...\")\n",
    "device_map = {\"\": 0}  # Use GPU 0\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    quantization_config=bnb_config, \n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Step 3: Define preprocessing function for clean instruction format\n",
    "def preprocess_function(examples):\n",
    "    # Use a clear instruction format without complex templates\n",
    "    formatted_prompts = [\n",
    "        f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {q}\\n\\n### Response:\" \n",
    "        for q in examples[\"Question\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenizer(\n",
    "            formatted_prompts,\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"Answer\"], \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"],\n",
    "        \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Step 4: Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.02,  # Reduced dropout for better learning\n",
    "    r=LORA_RANK,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target projection layers in OPT model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for kbit training\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.05,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    # Add the following to prevent repetition during training\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Data collator for language model training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 6: Create and train the model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    dataset_text_field=\"inputs_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the trained model\n",
    "os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Step 8: Test the model on a few examples\n",
    "print(\"Testing model on examples...\")\n",
    "\n",
    "# Load rouge for evaluation\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Reload base model for comparison\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Load fine-tuned model (PEFT)\n",
    "print(\"Loading fine-tuned model...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    FINAL_MODEL_PATH,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# Test on examples from test set\n",
    "test_questions = test_df['Question'][:10].tolist()  # Test on 10 examples\n",
    "test_answers = test_df['Answer'][:10].tolist()\n",
    "\n",
    "base_model_outputs = []\n",
    "peft_model_outputs = []\n",
    "\n",
    "print(\"\\nGenerating responses from base and fine-tuned models...\")\n",
    "for question in test_questions:\n",
    "    # Format prompt for the model\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    \n",
    "    # Base model generation\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    base_outputs = base_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,  # Increased repetition penalty\n",
    "        no_repeat_ngram_size=3   # Prevent repeating 3-grams\n",
    "    )\n",
    "    base_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the base model output\n",
    "    base_text = base_text.replace(prompt, \"\").strip()\n",
    "    base_text = post_process_response(base_text)\n",
    "    base_model_outputs.append(base_text)\n",
    "    \n",
    "    # Fine-tuned model generation\n",
    "    ft_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 250, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the fine-tuned model output\n",
    "    ft_text = ft_text.replace(prompt, \"\").strip()\n",
    "    ft_text = post_process_response(ft_text)\n",
    "    peft_model_outputs.append(ft_text)\n",
    "\n",
    "# Print results for a few examples\n",
    "for i, (question, answer, base_output, peft_output) in enumerate(zip(test_questions[:3], test_answers[:3], base_model_outputs[:3], peft_model_outputs[:3])):\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {answer}\")\n",
    "    print(f\"Base Model Output: {base_output}\")\n",
    "    print(f\"Fine-tuned Model Output: {peft_output}\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "base_rouge_results = rouge.compute(\n",
    "    predictions=base_model_outputs,\n",
    "    references=test_answers[:len(base_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "peft_rouge_results = rouge.compute(\n",
    "    predictions=peft_model_outputs,\n",
    "    references=test_answers[:len(peft_model_outputs)],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(\"Base Model:\")\n",
    "print(base_rouge_results)\n",
    "print(\"\\nFine-tuned Model:\")\n",
    "print(peft_rouge_results)\n",
    "\n",
    "# Save the generated responses for manual inspection\n",
    "results_df = pd.DataFrame({\n",
    "    \"Question\": test_questions,\n",
    "    \"Reference_Answer\": test_answers[:len(test_questions)],\n",
    "    \"Base_Model_Output\": base_model_outputs,\n",
    "    \"Fine_Tuned_Output\": peft_model_outputs\n",
    "})\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"\\nSaved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "# Create a simple inference function to test the model interactively\n",
    "def query_model(question, model=peft_model):\n",
    "    prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_length=len(input_ids[0]) + 300, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Clean the response\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    response = post_process_response(response)\n",
    "    return response\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "print(\"Example: response = query_model('What is the processing time for a green card application?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found at ./immigration_qa_dataset_clean, loading...\n",
      "Train size: 340, Validation size: 42, Test size: 43\n",
      "Loading tokenizer for facebook/opt-1.3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model found. Proceeding to evaluation...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 7.86MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 4.35MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 9.66MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.81k/6.81k [00:00<00:00, 10.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR metric could not be loaded or computed. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 10.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER metric could not be loaded or computed. Skipping.\n",
      "\n",
      "Evaluating Fine-tuned Model...\n",
      "METEOR metric could not be loaded or computed. Skipping.\n",
      "WER metric could not be loaded or computed. Skipping.\n",
      "\n",
      "--- Comprehensive Evaluation Results ---\n",
      "\n",
      "Base Model Metrics:\n",
      "rouge_rouge1: 0.22484908760582661\n",
      "rouge_rouge2: 0.022768048744514623\n",
      "rouge_rougeL: 0.10347650150268695\n",
      "rouge_rougeLsum: 0.10461042494637657\n",
      "tfidf_cosine_similarity: 0.17600273056830965\n",
      "bleu: 0.0077465861856109925\n",
      "perplexity: 30.44202381033492\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "rouge_rouge1: 0.24380942214897616\n",
      "rouge_rouge2: 0.025924972632562833\n",
      "rouge_rougeL: 0.1133994902769046\n",
      "rouge_rougeLsum: 0.11344463427783627\n",
      "tfidf_cosine_similarity: 0.21276319254850184\n",
      "bleu: 0.007051490908977086\n",
      "perplexity: 30.44202381033492\n",
      "\n",
      "Saved metrics results to ./model_evaluation_metrics.csv\n",
      "Saved comparison results to ./model_comparison_results.csv\n",
      "\n",
      "--- Example Outputs ---\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: If you receive NOIDs in your e-mail or online, it is important that you respond immediately so we have time to review and process them before they become final orders against you. The best way for us to address these types of requests is through our online Form 888, Request For Information. We will only send one response per request – if you don’t complete the form by 11:59 p.m. Eastern Time on Monday, Nov. 23, 2020, your request may not be considered within seven days as long as there remains no action pending at the USCIS offices where the application was submitted. Please refer to the instructions above regarding whether you must submit additional documents with your application; however, please note that if you submit multiple forms with your petition, we cannot consider all applications together until we determine which ones need further processing first. Additionally, once a Noid has been issued, you should never reissue your application because filing new petitions without submitting supplemental evidence could result in the cancellation of your original application, even though the status bar does remain active during the adjudication period. However, since we often require updated information when reviewing existing cases, we encourage you to file new applications while we continue to process old applications, ensuring\n",
      "Fine-tuned Model Output: If you receive a NOID, your attorney should contact USCIS immediately and ask for additional guidance on whether or not they will grant you TPS status. We typically accept denials as valid claims that must be followed up with further consideration by the agency in order to determine if we have sufficient evidence supporting our denial decision. However, we generally consider a denial request when it is accompanied by a Form I-765 requesting additional guidance; please refer to the instructions at the bottom of the form regarding submission requirements before submitting any documents to us. Please note that even though we may reject your application based upon a lack of adequate evidence supporting your claim for asylum, there are no immediate consequences associated with such rejection. Therefore, we strongly encourage you to submit all relevant documentation including, but not limited to, documentary proof of identity, employment history, biometric data, etc., within 30 days after receiving the NOID so that we can review your petition prior to processing. Once submitted, your document(s) will remain confidential until final approval has been granted. Failure to comply with these guidelines could result in the cancellation of your DACA/TPS designation which would result in serious financial penalties. This applies regardless of what type of denial you are given because once the\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: Yes, you may apply to receive your driver’s license if you meet the eligibility requirements listed in 8 CFR 212.3(b)(2). We will determine whether you can get a drivers' license based on several factors including age and criminal history record. Please see our Eligibility Guidelines section below or contact us at 1-866-924-2221 (TTY) for more details. For example, we consider how old you must be before applying for a drivers’ license; if there is any evidence of violent crime on your driving records such as convictions that involve force or use of deadly weapons against others; if you have been convicted of DUI with a blood alcohol content level over 0.08 percent or other dangerous offenses involving bodily injury; and if there was physical damage done by another person when you were behind the wheel of their vehicle. If you have not met these eligibility criteria, please do not submit an application requesting a driver‘s license until we review your case individually. Contact us today! ### About This Question: m Is it legal to drive without a valid driver“s license?\n",
      "Yes, legally driving while uninsured, underinsured, or unlicensed is illegal. Driving without a state issued ID\n",
      "Fine-tuned Model Output: Under the State and Local Law Enforcement Officers Act of 1988, it is illegal to drive without a license or registration in California. If you do not have legal authorization to obtain a driver’s license at age 16-18 years old, we will issue a temporary nonimmigrant visa (TNE) allowing you to remain in the United States while you await your final determination on whether you can gain lawful permanent residency status through our deferred action program. TNE holders may apply for their first Green Card once they complete their initial stay; however, if approved by DHS, they cannot work as fulltime employees until their green card application is pending. In addition, unless granted permission from USCIS to travel abroad with immediate family members under certain circumstances, those traveling outside the country must register before entering foreign countries. To be considered legally authorized to receive such permits, applicants should submit proof that they meet all other requirements established by law. Examples include documents showing they met federal income tax filing guidelines within the past six months, having no outstanding warrants, being enrolled in school or receiving government assistance, or completing a background check. For more information regarding eligibility criteria for obtaining a driving permit, see www.uscis.gov/immigration/drivers/. This type of documentation\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A Form I-819 is required for any immigrant who intends to acquire or use an EAD, even if the alien already has U.S.-issued social security numbers (SSN). This form establishes that you will not be able to receive a green card until your application is approved by USCIS. The only exception to this rule would be individuals with valid SSNs whose applications have been denied due to national origin discrimination under 8 CFR 221.21(b)(2) – (d), or who were determined ineligible for the Employment Authorization Act (EAA) employment authorization number as described in Section 1222a of Title 11, Code of Federal Regulations; see 8 C.F.R. §221.23(e); 8 CFR 220.225 – 220.230. These individuals may file an amended Form I‑819 without having first acquired an EAd from another source. Note that all forms must include sufficient documentation demonstrating eligibility before filing an amendment on an existing Form I‐918. Individuals seeking reentry status after removal should submit their Form I–819 together with the original Form I\\-1, Application for Adjustment of Status, at least six months prior to requesting reinstatement as a lawful permanent resident. If\n",
      "Fine-tuned Model Output: A person who is not a citizen of the United States or whose citizenship status has been revoked by order of U.S. law may request for their alien registration number to be issued as a social security number, provided they meet certain requirements in accordance with 8 CFR 230.21-230.24. To receive a social insurance number from USCIS, you must have completed Form I-485 (Forms I-129, I-130) and paid all fees associated with filing those applications. Once your application has received approval on form I-131, you will submit the appropriate fee receipt(s), along with supporting documentation including evidence that you pay taxes regularly through payroll deductions at work such as W-2’s and other forms related to wages earned while employed; if applicable, copies of tax records showing payments made to employers based on employment relationship between yourself and employer. Additionally, we encourage applicants under age 18 to apply online so that documents submitted during the application process do not require additional processing time when it comes back up later. Applicants without access to computer equipment may contact us using email at info@uscis.gov. We also provide help for people who cannot afford to file electronically but want assistance submitting electronic filings via fax machine.\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_model_final\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_final\"\n",
    "RESULTS_CSV = \"./model_comparison_results.csv\"\n",
    "METRICS_CSV = \"./model_evaluation_metrics.csv\"\n",
    "\n",
    "# Open-access model that doesn't require authentication\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # 1.3B parameters, open access\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== ENHANCED EVALUATION FUNCTIONS ====================\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of texts using the given model.\n",
    "    Lower perplexity indicates better predictive performance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = math.exp(loss.item())\n",
    "            total_perplexity += perplexity\n",
    "    \n",
    "    # Return average perplexity across all texts\n",
    "    return total_perplexity / len(texts)\n",
    "\n",
    "def calculate_tfidf_cosine_similarity(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between predictions and references using TF-IDF.\n",
    "    Higher values indicate more similar content.\n",
    "    \"\"\"\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Combine all texts to fit the vectorizer\n",
    "    all_texts = predictions + references\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    # Transform texts to TF-IDF vectors\n",
    "    pred_vectors = vectorizer.transform(predictions)\n",
    "    ref_vectors = vectorizer.transform(references)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for i in range(len(predictions)):\n",
    "        pred_vector = pred_vectors[i:i+1]  # Get the i-th prediction vector\n",
    "        ref_vector = ref_vectors[i:i+1]    # Get the i-th reference vector\n",
    "        similarity = cosine_similarity(pred_vector, ref_vector)[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Return average similarity\n",
    "    return sum(similarities) / len(similarities)\n",
    "\n",
    "def calculate_bleu_score(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for predictions against references.\n",
    "    Higher scores indicate better overlap in n-grams.\n",
    "    \"\"\"\n",
    "    bleu = evaluate.load('bleu')\n",
    "    \n",
    "    # Format references as list of lists (BLEU expects multiple references format)\n",
    "    formatted_references = [[ref] for ref in references]\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    results = bleu.compute(predictions=predictions, references=formatted_references)\n",
    "    \n",
    "    return results['bleu']\n",
    "\n",
    "def calculate_meteor_score(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate METEOR score for predictions against references.\n",
    "    METEOR is a metric that considers synonyms and paraphrasing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        meteor = evaluate.load('meteor')\n",
    "        results = meteor.compute(predictions=predictions, references=references)\n",
    "        return results['meteor']\n",
    "    except:\n",
    "        print(\"METEOR metric could not be loaded or computed. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def calculate_word_error_rate(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate between predictions and references.\n",
    "    Lower WER indicates fewer word-level differences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wer = evaluate.load('wer')\n",
    "        results = wer.compute(predictions=predictions, references=references)\n",
    "        return results\n",
    "    except:\n",
    "        print(\"WER metric could not be loaded or computed. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model_comprehensive(model, tokenizer, questions, reference_answers, \n",
    "                                model_name=\"Model\", max_length=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of model outputs using multiple metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    generated_outputs = []\n",
    "    \n",
    "    # Generate responses from model\n",
    "    for question in questions:\n",
    "        # Format prompt for the model\n",
    "        prompt = f\"### Instruction: You are an immigration assistant. Provide accurate information about this question: {question}\\n\\n### Response:\"\n",
    "        \n",
    "        # Generate output\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).input_ids.to(model.device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_length=len(input_ids[0]) + 250, \n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        # Decode and clean output\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        text = text.replace(prompt, \"\").strip()\n",
    "        text = post_process_response(text)\n",
    "        generated_outputs.append(text)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. ROUGE scores\n",
    "    rouge = evaluate.load('rouge')\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_outputs,\n",
    "        references=reference_answers,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    metrics.update({f\"rouge_{k}\": v for k, v in rouge_results.items()})\n",
    "    \n",
    "    # 2. TF-IDF Cosine Similarity\n",
    "    cos_sim = calculate_tfidf_cosine_similarity(generated_outputs, reference_answers)\n",
    "    metrics['tfidf_cosine_similarity'] = cos_sim\n",
    "    \n",
    "    # 3. BLEU Score\n",
    "    bleu = calculate_bleu_score(generated_outputs, reference_answers)\n",
    "    metrics['bleu'] = bleu\n",
    "    \n",
    "    # 4. METEOR Score (if available)\n",
    "    meteor = calculate_meteor_score(generated_outputs, reference_answers)\n",
    "    if meteor is not None:\n",
    "        metrics['meteor'] = meteor\n",
    "    \n",
    "    # 5. Word Error Rate (if available)\n",
    "    wer = calculate_word_error_rate(generated_outputs, reference_answers)\n",
    "    if wer is not None:\n",
    "        metrics['wer'] = wer\n",
    "    \n",
    "    # 6. Perplexity (on reference answers)\n",
    "    try:\n",
    "        perplexity = calculate_perplexity(model, tokenizer, reference_answers)\n",
    "        metrics['perplexity'] = perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating perplexity: {e}\")\n",
    "        metrics['perplexity'] = None\n",
    "    \n",
    "    return metrics, generated_outputs\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "def main():\n",
    "    # Check if dataset already exists\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f\"Dataset found at {DATASET_PATH}, loading...\")\n",
    "        dataset_dict = DatasetDict.load_from_disk(DATASET_PATH)\n",
    "        \n",
    "        # Convert to DataFrames for later use\n",
    "        train_df = pd.DataFrame(dataset_dict['train'])\n",
    "        val_df = pd.DataFrame(dataset_dict['validation'])\n",
    "        test_df = pd.DataFrame(dataset_dict['test'])\n",
    "        \n",
    "        print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "    else:\n",
    "        # Step 1: Load and prepare data\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_qa_data)\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(\"Sample data:\")\n",
    "        print(df.head(2))\n",
    "\n",
    "        # Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "        train_size = int(0.8 * len(df))\n",
    "        val_size = int(0.1 * len(df))\n",
    "\n",
    "        train_df = df[:train_size]\n",
    "        val_df = df[train_size:train_size+val_size]\n",
    "        test_df = df[train_size+val_size:]\n",
    "\n",
    "        print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "        # Convert to Hugging Face datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        # Combine into a dataset dictionary\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "\n",
    "        # Save the clean dataset to disk\n",
    "        os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "        dataset_dict.save_to_disk(DATASET_PATH)\n",
    "        print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "    # Step 2: Load Model and Tokenizer\n",
    "    # Define quantization config for 4-bit precision\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # For evaluation only - checking if trained model exists\n",
    "    if os.path.exists(FINAL_MODEL_PATH):\n",
    "        print(\"Trained model found. Proceeding to evaluation...\")\n",
    "        \n",
    "        # Load base model for comparison\n",
    "        print(\"Loading base model for comparison...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": 0}\n",
    "        )\n",
    "\n",
    "        # Load fine-tuned model (PEFT)\n",
    "        print(\"Loading fine-tuned model...\")\n",
    "        peft_model = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            FINAL_MODEL_PATH,\n",
    "            device_map={\"\": 0}\n",
    "        )\n",
    "\n",
    "        # Test on examples from test set\n",
    "        num_test_examples = min(15, len(test_df))  # Use at most 15 examples to keep evaluation time reasonable\n",
    "        test_questions = test_df['Question'][:num_test_examples].tolist()\n",
    "        test_answers = test_df['Answer'][:num_test_examples].tolist()\n",
    "\n",
    "        # Perform comprehensive evaluation\n",
    "        base_metrics, base_outputs = evaluate_model_comprehensive(\n",
    "            base_model, tokenizer, test_questions, test_answers, model_name=\"Base Model\"\n",
    "        )\n",
    "        \n",
    "        ft_metrics, ft_outputs = evaluate_model_comprehensive(\n",
    "            peft_model, tokenizer, test_questions, test_answers, model_name=\"Fine-tuned Model\"\n",
    "        )\n",
    "\n",
    "        # Save and display results\n",
    "        print(\"\\n--- Comprehensive Evaluation Results ---\")\n",
    "        print(\"\\nBase Model Metrics:\")\n",
    "        for k, v in base_metrics.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Metrics:\")\n",
    "        for k, v in ft_metrics.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Metric': list(base_metrics.keys()),\n",
    "            'Base Model': list(base_metrics.values()),\n",
    "            'Fine-tuned Model': list(ft_metrics.values()),\n",
    "            'Difference': [ft_metrics[k] - base_metrics[k] if isinstance(base_metrics[k], (int, float)) and \n",
    "                          isinstance(ft_metrics[k], (int, float)) else None \n",
    "                          for k in base_metrics.keys()]\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        metrics_df.to_csv(METRICS_CSV, index=False)\n",
    "        print(f\"\\nSaved metrics results to {METRICS_CSV}\")\n",
    "\n",
    "        # Save examples for qualitative analysis\n",
    "        examples_df = pd.DataFrame({\n",
    "            \"Question\": test_questions,\n",
    "            \"Reference_Answer\": test_answers,\n",
    "            \"Base_Model_Output\": base_outputs,\n",
    "            \"Fine_Tuned_Output\": ft_outputs\n",
    "        })\n",
    "        \n",
    "        examples_df.to_csv(RESULTS_CSV, index=False)\n",
    "        print(f\"Saved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "        # Print a few examples for quick reference\n",
    "        print(\"\\n--- Example Outputs ---\")\n",
    "        for i in range(min(3, len(test_questions))):\n",
    "            print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "            print(f\"Question: {test_questions[i]}\")\n",
    "            print(f\"Reference Answer: {test_answers[i]}\")\n",
    "            print(f\"Base Model Output: {base_outputs[i]}\")\n",
    "            print(f\"Fine-tuned Model Output: {ft_outputs[i]}\")\n",
    "\n",
    "        print(\"\\nEvaluation complete!\")\n",
    "    else:\n",
    "        print(f\"Trained model not found at {FINAL_MODEL_PATH}. Please train the model first.\")\n",
    "        # You can add training code here if needed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: scikit-learn in /home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found at ./immigration_qa_dataset_clean, loading...\n",
      "Train size: 340, Validation size: 42, Test size: 43\n",
      "Loading tokenizer for facebook/opt-1.3b...\n",
      "Loading model...\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 1539.55 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 1280.10 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 613.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 340\n",
      "Processed validation dataset size: 42\n",
      "Processed test dataset size: 43\n",
      "Preparing model for training...\n",
      "trainable params: 12,582,912 || all params: 724,361,216 || trainable%: 1.7371045994820353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 340/340 [00:00<00:00, 2065.20 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 2798.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 3/90 [00:40<19:29, 13.44s/it]\n",
      "  3%|▎         | 3/90 [00:51<19:29, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.643470525741577, 'eval_runtime': 1.5252, 'eval_samples_per_second': 27.538, 'eval_steps_per_second': 1.311, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 7/90 [01:39<19:34, 14.15s/it]\n",
      "  8%|▊         | 7/90 [01:44<19:34, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.532104015350342, 'eval_runtime': 1.6905, 'eval_samples_per_second': 24.845, 'eval_steps_per_second': 1.183, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 11%|█         | 10/90 [02:21<18:27, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6269, 'learning_rate': 1.982973099683902e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11/90 [02:34<17:47, 13.52s/it]\n",
      " 12%|█▏        | 11/90 [02:37<17:47, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4275171756744385, 'eval_runtime': 1.7721, 'eval_samples_per_second': 23.701, 'eval_steps_per_second': 1.129, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 15/90 [03:28<16:34, 13.26s/it]\n",
      " 17%|█▋        | 15/90 [03:30<16:34, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.342378616333008, 'eval_runtime': 1.5153, 'eval_samples_per_second': 27.717, 'eval_steps_per_second': 1.32, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 20%|██        | 18/90 [04:11<16:34, 13.81s/it]\n",
      " 20%|██        | 18/90 [04:24<16:34, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.28171706199646, 'eval_runtime': 1.7841, 'eval_samples_per_second': 23.541, 'eval_steps_per_second': 1.121, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 20/90 [04:45<17:30, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4507, 'learning_rate': 1.8502171357296144e-05, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [05:13<16:21, 14.44s/it]\n",
      " 24%|██▍       | 22/90 [05:18<16:21, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2095980644226074, 'eval_runtime': 1.7918, 'eval_samples_per_second': 23.441, 'eval_steps_per_second': 1.116, 'epoch': 5.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 26/90 [06:08<14:41, 13.78s/it]\n",
      " 29%|██▉       | 26/90 [06:11<14:41, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.135554313659668, 'eval_runtime': 1.571, 'eval_samples_per_second': 26.734, 'eval_steps_per_second': 1.273, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 30/90 [07:02<13:21, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3074, 'learning_rate': 1.6026346363792565e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 30/90 [07:04<13:21, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.05501651763916, 'eval_runtime': 1.5221, 'eval_samples_per_second': 27.593, 'eval_steps_per_second': 1.314, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 33/90 [07:45<13:11, 13.89s/it]\n",
      " 37%|███▋      | 33/90 [07:57<13:11, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9921693801879883, 'eval_runtime': 1.7772, 'eval_samples_per_second': 23.633, 'eval_steps_per_second': 1.125, 'epoch': 8.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 41%|████      | 37/90 [08:45<12:39, 14.33s/it]\n",
      " 41%|████      | 37/90 [08:50<12:39, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9160311222076416, 'eval_runtime': 1.5673, 'eval_samples_per_second': 26.798, 'eval_steps_per_second': 1.276, 'epoch': 9.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 40/90 [09:27<11:42, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1746, 'learning_rate': 1.2736629900720832e-05, 'epoch': 10.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 41/90 [09:40<11:15, 13.78s/it]\n",
      " 46%|████▌     | 41/90 [09:43<11:15, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.864976406097412, 'eval_runtime': 1.618, 'eval_samples_per_second': 25.958, 'eval_steps_per_second': 1.236, 'epoch': 10.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 45/90 [10:35<10:02, 13.38s/it]\n",
      " 50%|█████     | 45/90 [10:37<10:02, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8212977647781372, 'eval_runtime': 1.7587, 'eval_samples_per_second': 23.882, 'eval_steps_per_second': 1.137, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 48/90 [11:18<09:44, 13.90s/it]\n",
      " 53%|█████▎    | 48/90 [11:30<09:44, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.814617395401001, 'eval_runtime': 1.6493, 'eval_samples_per_second': 25.465, 'eval_steps_per_second': 1.213, 'epoch': 12.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 50/90 [11:52<10:01, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0727, 'learning_rate': 9.07731640536698e-06, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 52/90 [12:18<08:52, 14.01s/it]\n",
      " 58%|█████▊    | 52/90 [12:23<08:52, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8092262744903564, 'eval_runtime': 1.5285, 'eval_samples_per_second': 27.477, 'eval_steps_per_second': 1.308, 'epoch': 13.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 56/90 [13:12<07:40, 13.53s/it]\n",
      " 62%|██████▏   | 56/90 [13:16<07:40, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8058291673660278, 'eval_runtime': 2.0522, 'eval_samples_per_second': 20.466, 'eval_steps_per_second': 0.975, 'epoch': 14.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 60/90 [14:07<06:39, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.024, 'learning_rate': 5.542616442234618e-06, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 60/90 [14:09<06:39, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8030222654342651, 'eval_runtime': 1.4225, 'eval_samples_per_second': 29.526, 'eval_steps_per_second': 1.406, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 63/90 [14:49<06:09, 13.68s/it]\n",
      " 70%|███████   | 63/90 [15:02<06:09, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8012328147888184, 'eval_runtime': 1.8131, 'eval_samples_per_second': 23.165, 'eval_steps_per_second': 1.103, 'epoch': 16.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 67/90 [15:51<05:30, 14.39s/it]\n",
      " 74%|███████▍  | 67/90 [15:56<05:30, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7986561059951782, 'eval_runtime': 1.9015, 'eval_samples_per_second': 22.088, 'eval_steps_per_second': 1.052, 'epoch': 17.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 70/90 [16:34<04:45, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0244, 'learning_rate': 2.6099108277934105e-06, 'epoch': 18.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 71/90 [16:46<04:22, 13.79s/it]\n",
      " 79%|███████▉  | 71/90 [16:49<04:22, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7970116138458252, 'eval_runtime': 1.8109, 'eval_samples_per_second': 23.192, 'eval_steps_per_second': 1.104, 'epoch': 18.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 75/90 [17:41<03:22, 13.47s/it]\n",
      " 83%|████████▎ | 75/90 [17:43<03:22, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7960962057113647, 'eval_runtime': 1.6475, 'eval_samples_per_second': 25.493, 'eval_steps_per_second': 1.214, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 78/90 [18:25<02:46, 13.89s/it]\n",
      " 87%|████████▋ | 78/90 [18:37<02:46, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7956740856170654, 'eval_runtime': 1.8622, 'eval_samples_per_second': 22.555, 'eval_steps_per_second': 1.074, 'epoch': 20.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 80/90 [18:57<02:28, 14.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9872, 'learning_rate': 6.752777059564431e-07, 'epoch': 21.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 82/90 [19:25<01:54, 14.29s/it]\n",
      " 91%|█████████ | 82/90 [19:30<01:54, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7953498363494873, 'eval_runtime': 1.6008, 'eval_samples_per_second': 26.237, 'eval_steps_per_second': 1.249, 'epoch': 21.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 86/90 [20:20<00:55, 13.81s/it]\n",
      " 96%|█████████▌| 86/90 [20:23<00:55, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7952032089233398, 'eval_runtime': 1.6013, 'eval_samples_per_second': 26.228, 'eval_steps_per_second': 1.249, 'epoch': 22.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "100%|██████████| 90/90 [21:15<00:00, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.996, 'learning_rate': 0.0, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 90/90 [21:16<00:00, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7951714992523193, 'eval_runtime': 1.6378, 'eval_samples_per_second': 25.644, 'eval_steps_per_second': 1.221, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [21:16<00:00, 14.19s/it]\n",
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1276.9845, 'train_samples_per_second': 7.988, 'train_steps_per_second': 0.07, 'train_loss': 2.1848822911580403, 'epoch': 24.0}\n",
      "Model saved to ./immigration_assistant_gemma_final\n",
      "\n",
      "Preparing for comprehensive evaluation...\n",
      "Loading base model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hailemicaelyimer/anaconda3/envs/immigration_assistant/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Model...\n",
      "\n",
      "Evaluating Fine-tuned Model...\n",
      "\n",
      "--- Comprehensive Evaluation Results ---\n",
      "\n",
      "Base Model Metrics:\n",
      "rouge_rouge1: 0.25512243234592313\n",
      "rouge_rouge2: 0.03536031362850581\n",
      "rouge_rougeL: 0.11965858769181872\n",
      "rouge_rougeLsum: 0.12181364238077827\n",
      "tfidf_cosine_similarity: 0.2294454273578281\n",
      "bleu: 0.012305708975260607\n",
      "perplexity: 24.340541513230185\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "rouge_rouge1: 0.24615175872918588\n",
      "rouge_rouge2: 0.03461837850739836\n",
      "rouge_rougeL: 0.11328537262858526\n",
      "rouge_rougeLsum: 0.11805270888834501\n",
      "tfidf_cosine_similarity: 0.2007021213417432\n",
      "bleu: 0.012487286713355066\n",
      "perplexity: 24.340541513230185\n",
      "\n",
      "Saved metrics results to ./gemma_model_evaluation_metrics.csv\n",
      "Saved comparison results to ./gemma_model_comparison_results.csv\n",
      "\n",
      "--- Example Outputs ---\n",
      "\n",
      "\n",
      "--- Example 1 ---\n",
      "Question: I received a Notice of Intent to Deny (NOID) my case from the government. What can I do?\n",
      "Reference Answer: Many times the government improperly concludes that a case is deniable. Our experienced attorneys have successfully resolved cases in which the government intends to deny the case. While results may vary depending upon fact patterns and a case cannot always be resolved, a consultation with an attorney may turn up another avenue of relief.\n",
      "Base Model Output: If you receive NOI, follow the instructions in your notice and submit all documents requested by USCIS within 30 days of receipt of the NOI for review by the Immigration Appeals Office (IAO). You will be required to appear before the IAO during its scheduled hearing dates if you have not already filed with DHS or USCIS prior to receiving the no-action letter that is provided after filing. A decision on your petition may take several months following processing at which time we will provide additional guidance regarding filing extensions and other processes as appropriate based upon available resources. Please note that when reviewing petitions submitted without supporting evidence such as letters or interviews, the agency’s final determination remains subject to judicial control under § 212(a)(3), and therefore there does not exist any “judicial review” process in relation to it. Thus, you should consider whether you need to file an appeal of denial of your petition. However, please also remember that despite the fact that you initially failed to meet the requirements set forth in § 212A(d)(2)(ii)(B), we do recognize some cases where delay occurred due to administrative actions taken by the Department related to the application itself rather than because of our failure to grant approval. We encourage\n",
      "Fine-tuned Model Output: You have until 30 days after receiving your NOID notice or 15 business days if you submitted all required documents, to submit additional information requested by USCIS regarding your case status and file a Form I-130 for the adjustment of status that was denied based on denials under Section 212(a)(1). If you receive more than one notice of intent to deny your petition filed with USCIS in any calendar year, you must submit all notices within 60 days after each such date. See “Form I-129” below. To find out whether we denied your petition because of failure to file or other reasons, see our Frequently Asked Questions section. A final denial is not automatic but depends upon many factors including the nature and circumstances surrounding your request, compliance history, documentation submitted when originally filing for an adjustment of Status, employment verification, evidence supporting an alleged job offer, credible allegations against you as a result of previous deportations, etc., which may be considered during adjudication process. The decision will also consider whether there were sufficient grounds established at the time of initial processing to support the determination that you violated § 212(c), or whether it is appropriate to grant the petitioner relief pursuant to § 213(b). An appeal of final denial\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: m I eligible for a Driver's License?\n",
      "Reference Answer: Pennsylvania publishes the requirements under PUB 195NC, Fact Sheet: Identification and Legal Presence Requirements for Non-United States Citizens. Other states have similar publications.\n",
      "Base Model Output: You must be at least 18 years old and have completed the required driver training course to get your license. If you were under 18 when you began driving, then you will need your parent or guardian’s permission to drive on their behalf while they complete their driver training requirements. They can also apply for you if needed; we recommend that parents make all decisions related to licensing issues with their child(ren). Contact DMV Customer Service at 1-800-DMV-FRAUD (1-800 FEDERAL DRIVER) for more details.\n",
      "If you meet these qualifications but still do not qualify as a minor, you may request your first learner’ s permit by completing the Form DS-214 and submitting it to our office within 60 days of turning 16 years old. To obtain a provisional learner's permit, you should submit Form DS 214 after attaining age 15. Your provisional driver's permit is valid until you turn 21. For more information, contact DMV Customer Services toll free at 800-DMVCARE (855-846-2223), Monday through Friday from 8 am – 5 pm EST. Click here to view downloadable forms for learners permits.\n",
      "Your CDL class time starts when you receive your\n",
      "Fine-tuned Model Output: Drivers licenses and ID cards do not require proof of citizenship or legal residency, but you must have the proper documentation to obtain one in order to drive legally (i.e., drivers license). The following documents are needed: your driver’s license; valid passport with six months validity and current birth certificate; Social Security card (if applicable); current U.S.-issued identification document showing that you were born in United States; and proof of identity including your social security number if required by law enforcement agencies like the Department of Homeland Security. If you cannot provide all these requirements, contact our office at 1-877-845-3635 as soon as possible so we can get it right for you! You will be provided additional assistance during the process until you meet all state licensing standards — check back here after receiving your license/ID card once you arrive in Florida.\n",
      "When should I apply for my new ID Card?\n",
      "If you plan on driving regularly within three years of being issued your new IDCard, then please submit your application before the expiration date printed on your existing driver‘s license is set. Your final renewal may take up to two weeks from when you first receive your letter informing you that you need to renew your\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "Question: Does a noncitizen need a Social Security Number? How can a noncitizen obtain a Social Security Number and card?\n",
      "Reference Answer: The Social Security Administration published a guide to Social Security Numbers for Noncitizens on its website or you may call SSA toll free at 1-800-325-0778 or 1-800-772-1213 (for the deaf or hard of hearing).\n",
      "Base Model Output: A person who is not U.S.-born or born in the United States but has permanent residence status, as defined by Section 212(a)(1) of Title 14 CFR Part 213 (or any successor regulation), may request that the Department of Homeland Security issue him/her a Social security number if he/she meets certain requirements under Section 1273d of Title 7 USC 841(b). The applicant must provide proof to DHS indicating his/ her full name, date of birth, nationality (if foreign-born), sex, citizenship, place of residency, marriage and domestic partner identification numbers for all family members with whom he/ she resides, employment information, address, telephone number, signature on official documents such as driver’s license applications and passports, and other required documentation. To be eligible for issuance of a social security number, applicants should have been naturalized into the United State through lawful means within the preceding five years; provided evidence showing receipt from the Secretary of State of his/ hers identity document issued during his/hers previous stay at least six months before application; shown completion of Form I-130 (Application for Permanent Residence); completed Form DS-360 (Form H-2B Visa Waiver Program Information Request);\n",
      "Fine-tuned Model Output: A person who is not authorized to enter the United States or its territories may request for identification by submitting Form I-765, Application for Alien Registration (Form U-130). This form must be submitted together with any supporting documentation required under Section 212(a)(2) of the Immigration and Nationality Act. You will also have to provide your full name, date of birth, place of birth in the country where you live, nationality, social security number number, address, and passport number at least 30 days before you submit the application. The Social Security Administration then issues the individual their driver’s license/ID card based on that evidence. For more information regarding obtaining a Social Safety Card visit http://www.socialsecurity.gov/sscardinfo/. If your status as an alien requires issuance of a Social Worker Identification Document, please contact our Service Centers.\n",
      "A: No. Under VWP rules, there is no one designated refugee whose removal would result from revocation of the\n",
      "\n",
      "Training and evaluation complete!\n",
      "\n",
      "You can now use the query_model() function to test your model interactively.\n",
      "Example: response = query_model('What is the processing time for a green card application?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Set paths\n",
    "BASE_DIRECTORY = \"/home/hailemicaelyimer/Desktop/immigration-assistant/frequently-asked-questions\"\n",
    "DATASET_PATH = \"./immigration_qa_dataset_clean\"\n",
    "OUTPUT_DIR = \"./immigration_assistant_gemma_model\"\n",
    "FINAL_MODEL_PATH = \"./immigration_assistant_gemma_final\"\n",
    "RESULTS_CSV = \"./gemma_model_comparison_results.csv\"\n",
    "METRICS_CSV = \"./gemma_model_evaluation_metrics.csv\"\n",
    "\n",
    "# Use OPT-1.3B as a fallback since it's already working for you\n",
    "MODEL_ID = \"facebook/opt-1.3b\"  # Use your original model that worked\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 6  # Use your original batch size\n",
    "LEARNING_RATE = 2e-5\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = 64\n",
    "\n",
    "# ==================== HELPER FUNCTIONS ====================\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing question/answer prefixes and extra whitespace.\"\"\"\n",
    "    # Remove \"Q.\" or \"Q#.\" prefixes from questions\n",
    "    text = re.sub(r'^Q\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove \"A.\" or \"A#.\" prefixes from answers\n",
    "    text = re.sub(r'^A\\.?\\s*\\d*\\.?\\s*', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_and_clean_json_files(directory_path):\n",
    "    \"\"\"Load and clean all JSON files in the directory.\"\"\"\n",
    "    json_files = glob.glob(os.path.join(directory_path, \"**/*.json\"), recursive=True)\n",
    "    \n",
    "    all_data = []\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                for item in data:\n",
    "                    question = item.get(\"question\", \"\").strip()\n",
    "                    answer = item.get(\"answer\", \"\").strip()\n",
    "                    \n",
    "                    # Skip items with empty answers or questions\n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean the texts\n",
    "                    question = clean_text(question)\n",
    "                    answer = clean_text(answer)\n",
    "                    \n",
    "                    # Skip very short answers (likely not useful)\n",
    "                    if len(answer) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        \"Question\": question,\n",
    "                        \"Answer\": answer\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(all_data)} clean question-answer pairs\")\n",
    "    return all_data\n",
    "\n",
    "def post_process_response(text):\n",
    "    \"\"\"Clean model outputs by removing repetitions and known artifacts.\"\"\"\n",
    "    # Remove irrelevant prefix text\n",
    "    if \"Question:\" in text and \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    # Split by lines and remove duplicates while preserving order\n",
    "    lines = text.split('\\n')\n",
    "    seen_texts = set()\n",
    "    unique_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip duplicate content\n",
    "        if line in seen_texts:\n",
    "            continue\n",
    "            \n",
    "        # Skip lines that are question-like\n",
    "        if line.lower().startswith((\"question:\", \"q:\", \"what is\", \"how do\", \"can i\")):\n",
    "            continue\n",
    "            \n",
    "        seen_texts.add(line)\n",
    "        unique_lines.append(line)\n",
    "    \n",
    "    # Join unique lines\n",
    "    processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    # If we filtered too much, return the original without duplicates\n",
    "    if len(processed_text) < 20 and len(text) > 20:\n",
    "        lines = text.split('\\n')\n",
    "        seen_texts = set()\n",
    "        unique_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip() and line.strip() not in seen_texts:\n",
    "                seen_texts.add(line.strip())\n",
    "                unique_lines.append(line)\n",
    "        processed_text = '\\n'.join(unique_lines)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# ==================== ENHANCED EVALUATION FUNCTIONS ====================\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of texts using the given model.\n",
    "    Lower perplexity indicates better predictive performance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_perplexity = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize input text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = math.exp(loss.item())\n",
    "            total_perplexity += perplexity\n",
    "    \n",
    "    # Return average perplexity across all texts\n",
    "    return total_perplexity / len(texts)\n",
    "\n",
    "def calculate_tfidf_cosine_similarity(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between predictions and references using TF-IDF.\n",
    "    Higher values indicate more similar content.\n",
    "    \"\"\"\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Combine all texts to fit the vectorizer\n",
    "    all_texts = predictions + references\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    # Transform texts to TF-IDF vectors\n",
    "    pred_vectors = vectorizer.transform(predictions)\n",
    "    ref_vectors = vectorizer.transform(references)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for i in range(len(predictions)):\n",
    "        pred_vector = pred_vectors[i:i+1]  # Get the i-th prediction vector\n",
    "        ref_vector = ref_vectors[i:i+1]    # Get the i-th reference vector\n",
    "        similarity = cosine_similarity(pred_vector, ref_vector)[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Return average similarity\n",
    "    return sum(similarities) / len(similarities)\n",
    "\n",
    "def calculate_bleu_score(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for predictions against references.\n",
    "    Higher scores indicate better overlap in n-grams.\n",
    "    \"\"\"\n",
    "    bleu = evaluate.load('bleu')\n",
    "    \n",
    "    # Format references as list of lists (BLEU expects multiple references format)\n",
    "    formatted_references = [[ref] for ref in references]\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    results = bleu.compute(predictions=predictions, references=formatted_references)\n",
    "    \n",
    "    return results['bleu']\n",
    "\n",
    "def evaluate_model_comprehensive(model, tokenizer, questions, reference_answers, \n",
    "                                model_name=\"Model\", max_length=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of model outputs using multiple metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    generated_outputs = []\n",
    "    \n",
    "    # Generate responses from model\n",
    "    for question in questions:\n",
    "        # Format prompt for the model - Using standard instruction format\n",
    "        prompt = f\"You are an immigration assistant. Provide accurate information about this question: {question}\\n\\nAnswer:\"\n",
    "        \n",
    "        # Generate output\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).input_ids.to(model.device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_length=len(input_ids[0]) + 250, \n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        # Decode and clean output\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        text = text.replace(prompt, \"\").strip()\n",
    "        text = post_process_response(text)\n",
    "        generated_outputs.append(text)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. ROUGE scores\n",
    "    rouge = evaluate.load('rouge')\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_outputs,\n",
    "        references=reference_answers,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    metrics.update({f\"rouge_{k}\": v for k, v in rouge_results.items()})\n",
    "    \n",
    "    # 2. TF-IDF Cosine Similarity\n",
    "    cos_sim = calculate_tfidf_cosine_similarity(generated_outputs, reference_answers)\n",
    "    metrics['tfidf_cosine_similarity'] = cos_sim\n",
    "    \n",
    "    # 3. BLEU Score\n",
    "    bleu = calculate_bleu_score(generated_outputs, reference_answers)\n",
    "    metrics['bleu'] = bleu\n",
    "    \n",
    "    # 4. Perplexity (on reference answers)\n",
    "    try:\n",
    "        perplexity = calculate_perplexity(model, tokenizer, reference_answers)\n",
    "        metrics['perplexity'] = perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating perplexity: {e}\")\n",
    "        metrics['perplexity'] = None\n",
    "    \n",
    "    return metrics, generated_outputs\n",
    "\n",
    "# ==================== MAIN SCRIPT ====================\n",
    "\n",
    "def main():\n",
    "    # Check if dataset already exists\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f\"Dataset found at {DATASET_PATH}, loading...\")\n",
    "        dataset_dict = DatasetDict.load_from_disk(DATASET_PATH)\n",
    "        \n",
    "        # Convert to DataFrames for later use\n",
    "        train_df = pd.DataFrame(dataset_dict['train'])\n",
    "        val_df = pd.DataFrame(dataset_dict['validation'])\n",
    "        test_df = pd.DataFrame(dataset_dict['test'])\n",
    "        \n",
    "        print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "    else:\n",
    "        # Step 1: Load and prepare data\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        all_qa_data = load_and_clean_json_files(BASE_DIRECTORY)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_qa_data)\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(\"Sample data:\")\n",
    "        print(df.head(2))\n",
    "\n",
    "        # Split into train, validation, and test sets (80%, 10%, 10%)\n",
    "        train_size = int(0.8 * len(df))\n",
    "        val_size = int(0.1 * len(df))\n",
    "\n",
    "        train_df = df[:train_size]\n",
    "        val_df = df[train_size:train_size+val_size]\n",
    "        test_df = df[train_size+val_size:]\n",
    "\n",
    "        print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "        # Convert to Hugging Face datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        # Combine into a dataset dictionary\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "\n",
    "        # Save the clean dataset to disk\n",
    "        os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "        dataset_dict.save_to_disk(DATASET_PATH)\n",
    "        print(f\"Dataset saved to {DATASET_PATH}\")\n",
    "\n",
    "    # Step 2: Load Model and Tokenizer\n",
    "    # Define quantization config for 4-bit precision (with fallback to 8-bit)\n",
    "    try:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "    except:\n",
    "        print(\"4-bit quantization not supported, falling back to 8-bit\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0\n",
    "        )\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(f\"Loading tokenizer for {MODEL_ID}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load model with quantization config\n",
    "    print(\"Loading model...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID, \n",
    "            quantization_config=bnb_config, \n",
    "            use_cache=False,\n",
    "            device_map={\"\": 0}  # Use GPU 0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model with quantization: {e}\")\n",
    "        print(\"Trying without quantization...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            use_cache=False,\n",
    "            device_map={\"\": 0}\n",
    "        )\n",
    "\n",
    "    # Step 3: Define preprocessing function for clean instruction format\n",
    "    def preprocess_function(examples):\n",
    "        # Use a simple instruction format\n",
    "        formatted_prompts = [\n",
    "            f\"You are an immigration assistant. Provide accurate information about this question: {q}\\n\\nAnswer:\" \n",
    "            for q in examples[\"Question\"]\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenizer(\n",
    "                formatted_prompts,\n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                padding=\"max_length\"\n",
    "            )[\"input_ids\"],\n",
    "            \"labels\": tokenizer(\n",
    "                examples[\"Answer\"], \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                padding=\"max_length\"\n",
    "            )[\"input_ids\"],\n",
    "            \"inputs_text\": [f\"{prompt} {answer}\" for prompt, answer in zip(formatted_prompts, examples[\"Answer\"])],\n",
    "        }\n",
    "\n",
    "    # Apply preprocessing to datasets\n",
    "    print(\"Preprocessing datasets...\")\n",
    "    processed_train_dataset = dataset_dict['train'].map(preprocess_function, batched=True)\n",
    "    processed_val_dataset = dataset_dict['validation'].map(preprocess_function, batched=True)\n",
    "    processed_test_dataset = dataset_dict['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "    print(f\"Processed train dataset size: {len(processed_train_dataset)}\")\n",
    "    print(f\"Processed validation dataset size: {len(processed_val_dataset)}\")\n",
    "    print(f\"Processed test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "    # Step 4: Configure LoRA for efficient fine-tuning\n",
    "    # Target specific modules for OPT architecture\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=0.02,\n",
    "        r=LORA_RANK,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # Target modules for OPT model\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "    )\n",
    "\n",
    "    # Prepare model for kbit training\n",
    "    print(\"Preparing model for training...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Step 5: Define training arguments\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.05,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        report_to=\"none\",\n",
    "        # Add the following to prevent repetition during training\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    # Data collator for language model training\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    # Step 6: Create and train the model\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=processed_train_dataset,\n",
    "        eval_dataset=processed_val_dataset,\n",
    "        dataset_text_field=\"inputs_text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "    trainer.model.save_pretrained(FINAL_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "    print(f\"Model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "    # Step 8: Evaluate the model\n",
    "    print(\"\\nPreparing for comprehensive evaluation...\")\n",
    "    \n",
    "    # Reload base model for comparison\n",
    "    print(\"Loading base model for comparison...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "\n",
    "    # Load fine-tuned model (PEFT)\n",
    "    print(\"Loading fine-tuned model...\")\n",
    "    peft_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        FINAL_MODEL_PATH,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "\n",
    "    # Test on examples from test set\n",
    "    num_test_examples = min(15, len(test_df))  # Use at most 15 examples\n",
    "    test_questions = test_df['Question'][:num_test_examples].tolist()\n",
    "    test_answers = test_df['Answer'][:num_test_examples].tolist()\n",
    "\n",
    "    # Perform comprehensive evaluation\n",
    "    base_metrics, base_outputs = evaluate_model_comprehensive(\n",
    "        base_model, tokenizer, test_questions, test_answers, model_name=\"Base Model\"\n",
    "    )\n",
    "    \n",
    "    ft_metrics, ft_outputs = evaluate_model_comprehensive(\n",
    "        peft_model, tokenizer, test_questions, test_answers, model_name=\"Fine-tuned Model\"\n",
    "    )\n",
    "\n",
    "    # Save and display results\n",
    "    print(\"\\n--- Comprehensive Evaluation Results ---\")\n",
    "    print(\"\\nBase Model Metrics:\")\n",
    "    for k, v in base_metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    print(\"\\nFine-tuned Model Metrics:\")\n",
    "    for k, v in ft_metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': list(base_metrics.keys()),\n",
    "        'Base Model': list(base_metrics.values()),\n",
    "        'Fine-tuned Model': list(ft_metrics.values()),\n",
    "        'Difference': [ft_metrics[k] - base_metrics[k] if isinstance(base_metrics[k], (int, float)) and \n",
    "                      isinstance(ft_metrics[k], (int, float)) else None \n",
    "                      for k in base_metrics.keys()]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    metrics_df.to_csv(METRICS_CSV, index=False)\n",
    "    print(f\"\\nSaved metrics results to {METRICS_CSV}\")\n",
    "\n",
    "    # Save examples for qualitative analysis\n",
    "    examples_df = pd.DataFrame({\n",
    "        \"Question\": test_questions,\n",
    "        \"Reference_Answer\": test_answers,\n",
    "        \"Base_Model_Output\": base_outputs,\n",
    "        \"Fine_Tuned_Output\": ft_outputs\n",
    "    })\n",
    "    \n",
    "    examples_df.to_csv(RESULTS_CSV, index=False)\n",
    "    print(f\"Saved comparison results to {RESULTS_CSV}\")\n",
    "\n",
    "    # Print a few examples for quick reference\n",
    "    print(\"\\n--- Example Outputs ---\")\n",
    "    for i in range(min(3, len(test_questions))):\n",
    "        print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "        print(f\"Question: {test_questions[i]}\")\n",
    "        print(f\"Reference Answer: {test_answers[i]}\")\n",
    "        print(f\"Base Model Output: {base_outputs[i]}\")\n",
    "        print(f\"Fine-tuned Model Output: {ft_outputs[i]}\")\n",
    "\n",
    "    print(\"\\nTraining and evaluation complete!\")\n",
    "\n",
    "    # Create a simple inference function to test the model interactively\n",
    "    def query_model(question, model=peft_model):\n",
    "        prompt = f\"You are an immigration assistant. Provide accurate information about this question: {question}\\n\\nAnswer:\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_length=len(input_ids[0]) + 300, \n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "        response = post_process_response(response)\n",
    "        return response\n",
    "\n",
    "    print(\"\\nYou can now use the query_model() function to test your model interactively.\")\n",
    "    print(\"Example: response = query_model('What is the processing time for a green card application?')\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "immigration_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
